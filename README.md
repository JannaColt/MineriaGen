# MINER√çA GEN√ìMICA 
## CALIDAD, ENSAMBLE Y ANOTACI√ìN

Contiene pipeline para ensamble y miner√≠a gen√≥mica de shotgun sequence con google colab

El protocolo de manera general:

![Diagrama de flujo_An√°lisis](https://user-images.githubusercontent.com/13104654/211899391-66c4a856-e193-44f2-baae-0ad84895b78a.png)

![PipelineA](https://user-images.githubusercontent.com/13104654/211914431-9c9197e0-58b6-4e54-a068-221093935a43.png)

![PipelineB](https://user-images.githubusercontent.com/13104654/211915131-852fbab8-fc34-4ba3-a3f7-d91135098e47.png)

Antes que nada tenemos que instalar las herramientas que usaremos en la nube.
Desde el cuaderno establecido primeramente se instalan todos los paquetes que se usar√°n y al final se monta el drive en el que se estar√° trabajando. Es preferible que esto se haga desde el inicio ya que cuando se instala un nuevo paquete se reinicia el entorno y lo que anteriormente llamamos ya no estar√° disponible (cada vez que se quiera hacer el procedimiento ya que el cuaderno/entorno se ha cerrado hay que instalar todo de nuevo y hacer el montaje del drive). 

## 1. Instalacion de Herramientas

Instalamos conda y lo llamamos para proceder con la instalaci√≥n de los dem√°s paquetes usando conda
```python
!pip install -q condacolab
import condacolab
condacolab.install()
```
Luego instalamos miniconda, para lo cual utilizamos un bloque que llame al shell: %%Shell, con lo cual estaremos utilizando los comandos que se usan cuando se utiliza el shell

```shell
%%shell
#Seguimos el pipeline de FranciscoZorrilla Metagem, el cual es para establecer relaciones metabolicas en metagenomas,
#sin embargo, los primeros pasos para el filtrado de calidad se pueden seguir tal cual

#instalar y ejecutar miniconda

sudo apt-get update
wget https://repo.continuum.io/Miniconda3-latest-Linux-x86_64.sh -O miniconda.sh;
bash miniconda.sh -b -p $HOME/miniconda
export PATH="$HOME/miniconda/bin:$PATH"
export PYTHONPATH="$PATH"
hash -r
conda config --set always_yes yes --set changeps1 no
conda update --q conda
```

Para realizar la instalaci√≥n de FastQC utilizamos el siguiente bloque 
```python
# Instalar FastQC para identificar problemas de calidad en los datasets. 
!conda install -c bioconda fastqc -y

```
Adem√°s podemos instalar fastp (puedes revisar la calidad ya sea utilizando fastqc o fastp, fastp tambi√©n sirve para preprocesar en lugar de trimmomatic y cutapad)

```python
# Instalar fastp para identificar problemas de calidad en los datasets. 
!conda install -c bioconda fastp -y
```


Ahora instalamos tambi√©n MultiQC, este agregar√° todos los an√°lisis de calidad de los datos 
```python

# Instalar Multiqc que puede agregar y sumar todos los QC de los datos y alinear log data en un solo archivo 
!pip install multiqc
```
En caso de que querramos utilizar trimmomatic y no solo fastp podemos instalarlo a su vez.

```python
# Instalar Trimmomatic una herramienta flexible de trimming de lecturas para datos de Illumina NGS data 
! conda install -c bioconda trimmomatic -y
```

En caso de que queramos trabajar con transcriptomas (RNASeq) podemos utilizar Kallisto, usando las opciones:
Pseudobam

  --pseudobam outputs all pseudoalignments to a file pseudoalignments.bam in the output directory. This BAM file contains the pseudoalignments in BAM format,   ordered by reads so that each pseudoalignment of a read is adjacent in the BAM file.

  A detailed description of the SAM output is here.
 y GenomeBam

  --genomebam constructs the pseudoalignments to the transcriptome, but projects the transcript alignments to genome coordinates, resulting in split-read alignments. When the --genomebam option is supplied at GTF file must be given with the --gtf option. The GTF file, which can be plain text or gzipped, translates transcripts into genomic coordinates. We recommend downloading a the cdna FASTA files and GTF files from the same data source. The --chromosomes option can provide a length of the genomic chromosomes, this option is not neccessary, but gives a more consistent BAM header, some programs may require this for downstream analysis. kallisto does not require the genome sequence to do pseudoalignment, but downstream tools such as genome browsers will probably need it.
  
Es un programa para cuantificar la abundancia de &#x1F534; **Transcritos** a partir de datos de RNA-Seq en masa y unicelulares, o m√°s generalmente de secuencias objetivo utilizando lecturas de secuenciaci√≥n de alto rendimiento. Se basa en la novedosa idea de la pseudoalineaci√≥n para determinar r√°pidamente la compatibilidad de las lecturas con los objetivos, sin necesidad de alineaci√≥n.


Una vista m√°s detallada de kallisto la podemos encontrar aqu√≠: [Manual Kallisto](http://pachterlab.github.io/kallisto/manual.html)

```python
# Puede que Kallisto no sea necesario: es para cuantificar abundancias de transcritos de datos de RNA-seq, o 
#o de forma mas general secuencias blanco usando high-throughput sequencing reads. Podemos omitirlo en este cuaderno 
! conda install -c bioconda kallisto -y
```

Para utilizar metagem tenemos que clonar el repositorio del autor. Metagem nos permite reconstruir modelos metab√≥licos directamente de metagenomas. Se resumir√° un poco m√°s adelante.
[Art√≠culo de Metagem](https://academic.oup.com/nar/article/49/21/e126/6382386)


```shell
%%shell
#clonar el repositorio de metagem y despues moverse al directorio

git clone https://github.com/franciscozorrilla/metaGEM.git
cd metaGEM

#establecer mamba y metagem
/root/miniconda/bin/conda create --quiet --prefix ./envs/mamba mamba -c conda-forge && source /root/miniconda/bin/activate envs/mamba
mamba env create --quiet --prefix ./envs/metagem -f envs/metaGEM_env.yml

#activar metagem en el ambiente conda e instalar las herramientas pip
source /root/miniconda/bin/activate envs/metagem
pip install --quiet --user memote carveme smetana 

#desactivar metagem y activar el ambiente mamba
source /root/miniconda/bin/deactivate && source /root/miniconda/bin/activate envs/mamba

#Establecer metawrap y prokka-roary
mamba env create --quiet --prefix ./envs/metawrap -f envs/metaWRAP_env.yml
mamba env create --quiet --prefix ./envs/prokkaroary -f envs/prokkaroary_env.yml 
```
## Instalacion de BWA y SAMtools

```python

!conda install -c bioconda bwa -y
!conda install -c bioconda samtools -y

```

## Instalacion de Megahit y SPAdes
```python
#Para instalar megahit
!conda install -c bioconda megahit

#para instalar SPAdes
!conda install -c bioconda spades=3.15.5 -y
```

## Instalacion de Quast

```python
#Para instalar Quast
!conda install -c bioconda quast
```

## 2. Paqueter√≠a Python

En caso de que necesitemos utilizar paquetes para graficar y para trabajar con archivos csv debemos importar los paquetes matplotlib y pandas

```python
import matplotlib.pyplot as plt
import pandas as pd #llamar pandas
```

## 3. Montar Drive en colab
Para montar el drive utilizamos el siguiente bloque, esto es necesario para poder utilizar los archivos que se guarden en el drive y para guardar outputs en el mismo:

```python
#Montar el drive para trabajar con los archivos dentro del drive
from google.colab import drive
drive.mount('/content/drive')

```
Ya establecida toda la paqueter√≠a e instalados todos los pipelines podemos comenzar, teniendo en cuenta que nuestros archivos de secuencias (Pair-End o Single-End) deben estar guardados en nuestro drive.


## 4. Comencemos explorando las reads:

Las Lecturas com√∫nmente se encuentran en formato FASTQ, muy similar al FASTA solo que contiene caracteres alfanum√©ricos que dan indicio de calidad asociada con cada nucle√≥tido. Una imagen de la estructura de un archivo fastq la podemos observar m√°s adelante en el apartado del pre-procesamiento.

[Estructura FASTQ](#5-pre-procesamiento-analisis-de-calidad-usando-fastqc-y-fastp)

Son Archivos (de texto o ficheros) muy grandes que no se pueden leer. Ciertos comandos te permiten observar aspectos clave (Head/tail/more - trabajando con ficheros en Linux). 

Para conocer un poco nuestras secuencias podemos utilizar script de bash. En este caso trabajamos con los siguientes archivos:


![Nuestro drive y la carpeta conteniendo las lecturas:](https://user-images.githubusercontent.com/13104654/204870524-c62fcf00-b097-4758-b0a8-406bee184927.png)

N√≥tese que los archivos est√°n comprimidos y se pueden seguir trabajando de esta manera.

### a) Explorando el contenido 

Utilizando el bash podemos observar encabezados y colas de archivos, con el comando zcat podemos trabajar con archivos comprimidos, en caso de no tenerlos comprimidos simplemente usamos cat. Como las lecturas son pair end, lo realizamos con ambos archivos.

```bash
#explorar el encabezado del archivo
%%bash
zcat /content/drive/MyDrive/Analisis_Posdoc/PR69/HA1AB3SS04_S4_L1_R1_001.fastq.gz | head
```
```shell
%%bash
zcat /content/drive/MyDrive/Analisis_Posdoc/PR69/HA1AB3SS04_S4_L1_R2_001.fastq.gz | head
```

```shell
#explorar el final para las lecturas R1 y R2
%%bash
zcat /content/drive/MyDrive/Analisis_Posdoc/PR69/HA1AB3SS04_S4_L1_R1_001.fastq.gz | tail -n 4
```
```shell

%%bash
zcat /content/drive/MyDrive/Analisis_Posdoc/PR69/HA1AB3SS04_S4_L1_R2_001.fastq.gz | tail -n 4
```

### b) Revision del numero y longitud de secuencias 

> (Tambi√©n revisamos que se encuentren pareados, esta parte se puede omitir ya que fastqc te muestra algunos de estos resultados)

El siguiente bloque permite revisar si los archivos est√°n pareados
```bash
##Para explorar que los archivos est√°n pareados
%%bash
zcat /content/drive/MyDrive/Analisis_Posdoc/PR69/HA1AB3SS04_S4_L1_R1_001.fastq.gz | wc -l
zcat /content/drive/MyDrive/Analisis_Posdoc/PR69/HA1AB3SS04_S4_L1_R2_001.fastq.gz | wc -l
```
si queremos saber cual es el n√∫mero de secuencias usamos el siguiente (aunque es inespec√≠fico), de nuevo usamos zgrep por que es un archivo comprimido.

```bash
##Explorar la cantidad de secuencias (deber√≠a ser #####, n√∫mero de l√≠neas/4)
%%bash
zgrep '^@' /content/drive/MyDrive/Analisis_Posdoc/PR69/HA1AB3SS04_S4_L1_R1_001.fastq.gz | wc -l
zgrep '^@' /content/drive/MyDrive/Analisis_Posdoc/PR69/HA1AB3SS04_S4_L1_R2_001.fastq.gz | wc -l
```
para hacer que el anterior bloque haga una b√∫squeda m√°s espec√≠fica tenemos

```bash
##Hay que ser m√°s espec√≠ficos para revisar la cantidad de secuencias
%%bash
zgrep '^@M02521' /content/drive/MyDrive/Analisis_Posdoc/PR69/HA1AB3SS04_S4_L1_R1_001.fastq.gz | wc -l
zgrep '^@M02521' /content/drive/MyDrive/Analisis_Posdoc/PR69/HA1AB3SS04_S4_L1_R2_001.fastq.gz | wc -l
```

Exploramos la longitud de secuencias. Para ello podemos usar awk, el cual es un comando (lenguaje de programaci√≥n) que te permite trabajar operaciones matem√°ticas con ficheros de texto (filtrando). M√°s acerca de awk lo puedes encontrar [aqu√≠](http://www.linuxfocus.org/English/September1999/article103.html#lfindex0) o [ac√°](https://geekland.eu/uso-del-comando-awk-en-linux-y-unix-con-ejemplos/).

Para cada l√≠nea de secuencia podemos contar cada caracter usando el par√°metro NR (n√∫mero de registros) y usando el contador.

```bash
%%bash
zcat /content/drive/MyDrive/Analisis_Posdoc/PR69/HA1AB3SS04_S4_L1_R1_001.fastq.gz | awk 'NR%4 == 2 {lengths[length($0)]++ ; counter++} END {for (l in lengths){print l, lengths[l]}; print "total reads: " counter}'

```
y podemos a√±adir par√°metros para imprimir en txt

```bash
%%bash
zcat /content/drive/MyDrive/Analisis_Posdoc/PR69/HA1AB3SS04_S4_L1_R1_001.fastq.gz | awk 'NR%4 == 2 {lengths[length($0)]++ ; counter++} END {for (l in lengths){print l, lengths[l]}}' | sort -n | uniq -c > /content/drive/MyDrive/Analisis_Posdoc/read_length1.txt
```

el txt generado lo podemos importar y transformar a csv para despu√©s graficarlo en matplotlib, para lo cual se pueden aplicar los siguientes bloques:

```python
read_file = pd.read_csv (r'/content/drive/MyDrive/Analisis_Posdoc/read_length1.txt',header=None)
read_file.to_csv (r'/content/drive/MyDrive/Analisis_Posdoc/read_lengthR1.csv', index=None,header=None)
```
Generar el df tipo decimal

```python
read_file[0] = read_file[0].astype(str)
df = read_file[0].apply(lambda x: x.strip(" "))
df = pd.DataFrame(df.str.split(' ',2).tolist(),
                                 columns = ['fips','length', 'ocurrences'])
df
df=df.astype(float)
```

y graficamos
```python
##Graficar la longitud de secuencia contra las ocurrencias
df.plot(x= 'ocurrences', y= 'length', kind ='line')
plt.show()
```
lo anterior lo aplicamos tambi√©n en Reverse 

```bash
%%bash
zcat /content/drive/MyDrive/Analisis_Posdoc/PR69/HA1AB3SS04_S4_L1_R2_001.fastq.gz | awk 'NR%4 == 2 {lengths[length($0)]++ ; counter++} END {for (l in lengths){print l, lengths[l]}; print "total reads: " counter}'
```

```bash
#Output de distribuci√≥n de longitudes de secuencias y graficar en matplotib
%%bash
zcat /content/drive/MyDrive/Analisis_Posdoc/PR69/HA1AB3SS04_S4_L1_R2_001.fastq.gz | awk 'NR%4 == 2 {lengths[length($0)]++ ; counter++} END {for (l in lengths){print l, lengths[l]}}' | sort -n | uniq -c > /content/drive/MyDrive/Analisis_Posdoc/read_length2.txt
```

```python
read_file2 = pd.read_csv (r'/content/drive/MyDrive/Analisis_Posdoc/read_length2.txt',header=None)
read_file2.to_csv (r'/content/drive/MyDrive/Analisis_Posdoc/read_lengthR2.csv', index=None,header=None)
```

```python
read_file2[0] = read_file2[0].astype(str)
df2 = read_file2[0].apply(lambda x: x.strip(" "))
df2 = pd.DataFrame(df2.str.split(' ',2).tolist(),
                                 columns = ['fips','length', 'ocurrences'])
df2
df2=df2.astype(float)
```

```python
##Graficar la longitud de secuencia contra las ocurrencias
graficaR2 = df2.plot(x= 'ocurrences', y= 'length', kind ='line')
plt.show()
plt.savefig("ReadlenghtR2.jpg", bbox_inches='tight')
```
Lo siguiente puede ser opcional.

### c) bloques para mostrar solo secuencias o secuencias repetidas

```bash
#Desplegar solo la informaci√≥n de la secuencia
%%bash
zcat /content/drive/MyDrive/Analisis_Posdoc/PR69/HA1AB3SS04_S4_L1_R1_001.fastq.gz | awk 'NR % 4 == 2 {print;}' | head -n 10
```

```bash
%%bash
zcat /content/drive/MyDrive/Analisis_Posdoc/PR69/HA1AB3SS04_S4_L1_R2_001.fastq.gz | awk 'NR % 4 == 2 {print;}' | head -n 10
```
podemos considerar un output txt para las secuencias repetidas

```bash
#Observar barcodes-secuencias que se presentan m√°s frecuentemente
%%bash
zcat /content/drive/MyDrive/Analisis_Posdoc/PR69/HA1AB3SS04_S4_L1_R1_001.fastq.gz | awk 'NR % 4 == 2 {print;}' | sort | uniq -c | sort -n -r | head -n 10
zcat /content/drive/MyDrive/Analisis_Posdoc/PR69/HA1AB3SS04_S4_L1_R1_001.fastq.gz | awk 'NR % 4 == 2 {print;}' | sort | sort -n -r | uniq -c > /content/drive/MyDrive/Analisis_Posdoc/Sec_rep1.txt
```
```bash
%%bash
zcat /content/drive/MyDrive/Analisis_Posdoc/PR69/HA1AB3SS04_S4_L1_R2_001.fastq.gz | awk 'NR % 4 == 2 {print;}' | sort | uniq -c | sort -n -r | head -n 10
zcat /content/drive/MyDrive/Analisis_Posdoc/PR69/HA1AB3SS04_S4_L1_R2_001.fastq.gz | awk 'NR % 4 == 2 {print;}' | sort | sort -n -r | uniq -c > /content/drive/MyDrive/Analisis_Posdoc/Sec_rep2.txt
```

## 5. Pre procesamiento: analisis de calidad usando FASTQC y FASTP

El archivo FastQ que hab√≠amos abordado [anteriormente](#4-comencemos-explorando-las-reads), 
consta de cuatro l√≠neas:

&#x1F535; 1. Nombre de la secuencia (header - id del secuenciador, coordenadas del spot, flow-cell, adaptador, etc.)

&#x1F535; 2. Secuencia

&#x1F535; 3. Espaciador (+), que opcionalmente puede contener m√°s texto, y

&#x1F535; 4. Valores de calidad: Q Score - alfanum√©rico. Correspondientes a los nucle√≥tidos en l√≠nea 2
    
Tal como se observa en la siguiente imagen:
![Estructura FastQ](https://user-images.githubusercontent.com/13104654/204933554-fef6cf9a-f8d4-4e52-ad1b-a831d5bfdd92.png)

Los scores de calidad (Q) representados en la cuarta l√≠nea, que corresponden a caracteres ASCII, reflejan, en escala logar√≠tmica, la probabilidad de que esta base en particular fuera llamada incorrectamente (P<sub>error</sub>).

```math
Q = -10 log_{10} P               
```
```math
P_{error} = 10^{-Q/10}
```
Lo cual corresponde generalmente a:

![Phred Quality score](https://user-images.githubusercontent.com/13104654/205389945-0d25371a-f02b-4db0-8e80-07e2cedf0e41.png)

Hay que considerar adem√°s el m√©todo de codificado dependiendo de plataforma:

![Quality scores dependiendo de la plataforma de secuenciaci√≥n](https://user-images.githubusercontent.com/13104654/205388609-2d6df438-0aea-4a9e-a612-37563d0e83e6.png)

As√≠, podemos adem√°s, aplicar otro c√≥digo para determinarr si el score de nuestras lecturas corresponde a Phred+33, Phre+64 o Solexa+64

```bash
%%bash
zcat /content/drive/MyDrive/Analisis_Posdoc/PR69/HA1AB3SS04_S4_L1_R1_001.fastq.gz | head -n 10000 |\
  awk '{if(NR%4==0) printf("%s",$0);}' |  od -A n -t u1 | \
  awk 'BEGIN{min=100;max=0;} \
      {for(i=1;i<=NF;i++) \
          {if($i>max) max=$i; \
               if($i<min) min=$i;}}END \
          {if(max<=74 && min<59) \
                     print "Phred+33"; \
           else \
           if(max>73 && min>=64) \
                     print "Phred+64"; \
           else \
           if(min>=59 && min<64 && max>73) \
                     print "Solexa+64"; else print "Unknown score encoding!";}' --- [Table of contents](/introduction/terminology_index.html)
```

Para entender un poco m√°s acerca de [calidades](https://maq.sourceforge.net/qual.shtml) y los archivos fastq podemos ir a la documentaci√≥n oficial de [FastQ](https://maq.sourceforge.net/fastq.shtml) y este [art√≠culo](https://pubmed.ncbi.nlm.nih.gov/9521921/), adem√°s de esta [nota t√©cnica](https://www.illumina.com/documents/products/technotes/technote_Q-Scores.pdf) de Illumina.


Tomando en cuenta lo anterior, podemos utilizar algunas herramientas para identificar problemas de calidad en las lecturas, con el fin no solo de mantener las secuencias adecuadas si no tambi√©n de reducir el tama√±o del archivo, evitar contaminaci√≥n, etc.

Entre las herramientas a utilzar tenemos:

# 5.1 Fastqc
Para correr FastQC en los archivos de secuencias dentro de google colab usamos el siguiente bloque de c√≥digo:

```python
# Pre-alignment QA 
!fastqc /content/drive/MyDrive/Analisis_Posdoc/PR69/HA1AB3SS04_S4_L1_R1_001.fastq.gz
!fastqc /content/drive/MyDrive/Analisis_Posdoc/PR69/HA1AB3SS04_S4_L1_R2_001.fastq.gz
```
Los archivos de salida son html y abarcan las siguientes evaluaciones:

## 5.1.1. Estad√≠stica simple

El primer apartado de estad√≠stica simple contiene el nombre del archivo, el n√∫mero total de secuencias, si existen secuencias de mala calidad, longitud de las secuencias y el contenido de GC. Ya se hab√≠a abordado en los scripts anteriores la determinaci√≥n del total y longitud de secuencias, por lo que nos sirve para corroborar. En este apartado no se generan warnings o fails.


![Panorama general y vista de los estad√≠sticos iniciales](https://user-images.githubusercontent.com/13104654/205708653-93a21bca-be14-44e7-839a-fba67d08786e.png)

## 5.1.2. Calidad de secuencias por base

Este es el valor de confianza de base con base en el Phred score que designa las series de score de calidad de las bases completas en su respectica locaci√≥n en el archivo. un valor m√°s all√° de Q30 es considerado bueno, mientras que uno arriba de Q20 es generalmente aceptado.
En este apartado se muestra una revisi√≥n del rango de los valores de calidad a trav√©s de todas las bases en cada posici√≥n en los archivos FASTQ. 
En cada posici√≥n se muestra una boxplot con bigotes. En la gr√°fica podemos definir una mediana (l√≠nea roja central), rangos intercuartiles 25-75%(cajas amarillas), los bigotes representan los puntos del 10 y 90% y la calidad media (l√≠nea azul). 

![calidad de secuencias por base para la lectura R1 de P69](https://user-images.githubusercontent.com/13104654/205745971-2a852136-b15d-431f-8720-de0edb5af83c.png)


El eje y corresponde a los *scores* de calidad, el cual es dividido con un fondo verde, naranja y rojo, siendo el fondo verde para los mejores *scores*, el naranja para los *scores* de no tan buena calidad y el rojo a los de calidad pobre (entre m√°s alto el *score* mejor). 

Es normal para todas las plataformas que conforme avance la corrida la calidad disminuya. En esta parte se puede generar un warning, en el caso de que el cuartil de cualquier base sea menor de 10 o si la mediana es menor de 25 y fails si el cuartil es menor de 5 y la mediana menor de 20.

En el an√°lisis que se realiz√≥ del genoma de P69 se muestra que la calidad media no cae al fondo rojo y su comportamiento es t√≠pico disminuyendo la calidad con la corrida. 
Se observa en la √∫ltima parte solo la mediana de una caja en el umbral de 20, y los cuartiles menores de 10 pero mayores de 5, por lo tanto solo se lanza un warning, el cual ser√° resuelto al realizar el preprocesamiento.  


## 5.1.3.  Calidad de secuencias por pozo (*flowcell*) 

![illumina_flowcell](https://user-images.githubusercontent.com/13104654/212420432-e0c77336-d557-4a28-a25e-6e03ef1ab5a8.png)

En este apartado se muestra un heatmap de las p√©rdidas de calidad posicional, es decir se grafica la calidad de las llamadas de bases contra la posici√≥n f√≠sica del secuenciador de la cual provienen. En los secuenciadores illumina, el √°rea del *flowcell* se divide artificialmente en franjas (superficie superior e inferior) y 
estas se subdividen en mosaicos (√°reas arbitrarias que se analizan por separado en la canalizaci√≥n de llamadas). Observar la calidad por mosaico identificar√° este tipo de errores. Se espera siempre la p√©rdida de calidad conforme los ciclos se incrementan, por ello resulta √∫til realizar una normalizaci√≥n para representar la calidad. As√≠ un buen gr√°fico se observar√° en blanco (s√≥lido azul rey brillante). 
De esta forma, los problemas se pueden manifestar de varias formas, en la prueba de P69:

![P69 calidad de secuencia por flowcell R1](https://user-images.githubusercontent.com/13104654/210288959-a6367307-2827-4ff1-b6ec-2dfb468564c0.png)

Puede haber una p√©rdida aleatoria de calidad en las posiciones y ciclos, lo cual indica un problema generalizado en el que m√°s com√∫nmente se sobrecarga la celda. En el an√°lisis de calidad de la imagen de la cepa P69 se observa algo de este problema generalizado con la corrida aunque algo menos intenso. Resulta un tanto problem√°tico si mosaicos aparecen en √°reas amplias y localizadas  del *flowcell*.

Si se pueden observar las p√©rdidas de calidad en mosaicos espec√≠ficos entonces es posible removerlos del an√°lisis *downstream*, lo que resultar√≠a algo problem√°tico de estar al inicio de la corrida. 

Ya que no sabemos cuantas lecturas son afectadas entonces la mitigaci√≥n en este caso (P69) podr√≠a resultar un problema (al remover lecturas se podr√≠a perder informaci√≥n).

## 5.1.4.  Scores de calidad por secuencia 
Este apartado muestra un gr√°fico del n√∫mero de secuencias (Y) contra la escala logar√≠tmica del Phred (X), indicando la calidad de cada lectura:

![Calidad por secuencia](https://user-images.githubusercontent.com/13104654/210292722-b817acd1-1c04-415f-b470-91bc1f2aa7fd.png)

Phred score = 30 indica un rango de error de 1 base en 1000, o una exactitud de 99.9%.
Phred score = 40 indica un rango de error de 1 base en 10,000, o una exactitud de 99.99%.
Si el Phred score es < 27 se obtendr√° un warning y por debajo de 20 se dar√° un fail. 

En el caso de P69, el promedio de calidad es 36, lo cual es bueno.

## 5.1.5.  Contenido de bases por secuencia

En el caso del contenido de bases por secuencia, este apartado nos muestra, como el nombre lo indica, la composici√≥n porcentual de las bases en cada posici√≥n de la secuencia. Como habr√≠a que esperar esta composici√≥n debe permanecer estable en todos los ciclos, claro tomando en cuenta que el contenido de bases puede variar dependiendo de ciertos factores como la especie. En algunos casos podemos observar un sesgo en las primeras partes de la corrida, como es el caso del presente an√°lisis P69. Se observa claramente que dicho sesgo se disipa en el resto de la corrida. 


![Composici√≥n de secuencia](https://user-images.githubusercontent.com/13104654/210295270-29332fcd-2e65-4814-9439-5f7f743b6ab8.png)


En este apartado se puede generar una  :warning: **alerta**, si el contenido de bases var√≠a m√°s del 10% en cualquier posici√≥n, y generar√° un :x: **fail** si este porcentaje de variaci√≥n es mayor al 20%.

La causa de este sesgo puede ser el paso de *priming* aleatorio en la producci√≥n de las librer√≠as. A pesar de que los hexameros en el priming deben presentarse con igual frecuencia en el mix y deber√≠an realizar el *prime* con eficiencia similar, en la realidad no se da el caso y ciertos hex√°meros son favorecidos durante este paso.

¬øEntonces, el sesgo tendr√≠a implicaciones en los an√°lisis downstream?. Hay algunos puntos a tomar en cuenta:

- Es posible que como parte del sesgo haya un incremento en el *mis-priming* - ocasionando un n√∫mero alto de *mis-called* bases al inicio de la secuencia, y, 
- Es posible que la selecci√≥n del sesgo introducido tenga un efecto significativo en la capacidad de la librer√≠a de medir el contenido original debido a ciertas secuencias favorecidas.

Sin embargo estos puntos pueden no representar un gran problema ya que son f√°cilmente detectados,  algunos mencionan que pueden mitigarse por un *Trimming 5'*, sin embargo esto no es un arreglo. Ya que la composici√≥n sesgada es creada por la selecci√≥n de fragmentos de secuenciado y no por errores de llamadas de bases, el √∫nico efecto del *trimming* es cambiar de tener una libreria que inicia en posiciones sesgadas a una que inicia m√°s all√° de dichas posiciones. 

La √∫nica forma de resolver este problema ser√≠a introducir nuevos kits de preparaci√≥n de librer√≠as con una menor disposici√≥n al sesgo en el paso del *priming*, sin embargo, a pesar de la advertencia,  no parece que haya consecuencias serias para los an√°lisis posteriores, ir√≥nicamente en RNA-seq son m√°s sospechosas las librer√≠as que no presentan este artefacto.

En este [art√≠culo](https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0085583) se documenta el *mis-priming* en RNA-seq

Algunas de las razones m√°s comunes de un warning o fail en este apartado son:
Secuencias sobrerepresentadas, sesgo en la fragmentaci√≥n y composici√≥n sesgada de las librer√≠as (que a veces ocurre naturalmente).

En el presente caso tambi√©n se observa una desviaci√≥n al final, si se est√° analizando una biblioteca que ha sido recortada agresivamente por el adaptador, naturalmente introducir√° un sesgo de composici√≥n al final de las lecturas, ya que las secuencias que coinciden con tramos cortos del adaptador se eliminan, dejando solo las secuencias que no coinciden. Por lo tanto, es probable que las desviaciones repentinas en la composici√≥n al final de las bibliotecas que han sufrido un recorte agresivo sean falsas.


## 5.1.6.  Contenido de GC por secuencia

Este apartado muestra en un plot, el contenido porcentual total de GC para todas las lecturas (n√∫mero total de reads vs porcentaje de bases G y C por lectura), comparando contra una "distribuci√≥n te√≥rica" de GC's, asumiendo un contenido uniforme para todas las lecturas, el pico central corresponde al contenido de GC promedio del genoma subyacente. Dado que el contenido de GC del genoma no se conoce, el contenido modal de GC es calculado de datos observados y usado para construir la distribuci√≥n de referencia.

![Contenido de GC](https://user-images.githubusercontent.com/13104654/210295287-8ed4d1b0-051f-498d-b5f9-d2cdbb5d60c3.png)

 Se observa un ‚ö†Ô∏è**warning** si el 15% total de las secuencias caen fuera de la distribuci√≥n normal.
 
 Se obtendr√° un :x:**fail** si m√°s del 20% (el manual FastQC indica 30%) de las secuencias est√°n fuera de la distribuci√≥n normal.
 Los fails son generalmente debidos a contaminaci√≥n, frecuentemente por secuencias de adaptadores.

Una distribuci√≥n de forma inusual podr√≠a indicar una librer√≠a contaminada o alguna otra clase de subset sesgado. Una distribuci√≥n normal cambiada √≠ndica alg√∫n sesgo sistem√°tico, el cual es independiente de la posici√≥n de la base. Si existe un error sistem√°tico, este no ser√° marcado como error por que no se sabe cual deber√≠a ser el contenido de GC del genoma.

Existen otras situaciones en las cuales una distribuci√≥n inusual se puede presentar. Por ejemplo, con RNA seq puede haber una distribuci√≥n mayor o menor del contenido medio de GC entre los transcritos, causando que el gr√°fico observado sea m√°s amplio o m√°s estrecho que una distribuci√≥n normal ideal.

## 5.1.7.  Contenido de N por base 

Si un secuenciador no puede llamar una base con confianza suficiente entonces ser√° sustituido normalmente con un N m√°s que una llamada de base convencional.

Este m√≥dulo gr√°fica el porcentaje de "llamadas" de base en cada posici√≥n para las cuales una N fue considerada.

![Contenido de N P69](https://user-images.githubusercontent.com/13104654/210295307-9831c8c6-f696-4640-ad40-0efc91a27528.png)

Idealmente el contenido de N por base ser√≠a una l√≠nea plana en 0% sobre el eje Y, indicando que todas las bases han sido "llamadas".

  - Se recibe un :warning: **warning** si el contenido de N es igual o mayor de 5%,
  - Tendremos un :x: **fail** si el contenido de N es igual o mayor a 20%.

El an√°lisis de R1 para P69 muestra el resultado ideal para este m√≥dulo.

## 5.1.8.  Distribuci√≥n de la longitud de secuencia

Este gr√°fico muestra, tal como hicimos en [apartados anteriores](#b-revision-del-numero-y-longitud-de-secuencias), la distribuci√≥n de los tama√±os de fragmentos en el archivo analizado. En muchos casos esto solo produce un gr√°fico simple mostrando solo un pico de un solo tama√±o, pero para archivos FASTQ con longitud variable, mostrara las cantidades relativas de cada tama√±o de fragmento de secuencia. En este caso nuestro archivo R1 P69 muestra longitudes variables m√°s peque√±as (30-269pb) que el pico de 299pb. 

![Longitud de secuencias R1 P69](https://user-images.githubusercontent.com/13104654/210295322-a0759d95-e0a6-42cc-b699-343741a5974e.png)


Algunos secuenciadores (y kits de secuenciaci√≥n) generan fragmentos de longitudes ampliamente variables, otros pueden generar fragmentos de longitud uniforme.
Incluso en librer√≠as con longitud uniforme, algunos *pipelines* cortar√°n secuencias para remover llamadas de bases de baja calidad del final o las primeras n bases si coninciden las primeras n bases del adapatador arriba del 90% (por defecto), algunas veces con n=1. 
Para secuenciaci√≥n con Illumina, cada lectura deber√≠a ser del mismo tama√±o (?). 

Este m√≥dulo arrojar√° un :warning:**warning** si hay cualquier variaci√≥n en la longitud de las secuencias, el cual puede ser ignorado si se sabe que es normal para el tipo de datos que se tiene. 

Un :x: **fail** en este m√≥dulo significa que al menos una secuencia tiene longitud de 0. 
El an√°lisis de R1 P69 obtiene un warning ya que hay una gran variabilidad en la longitud de las secuencias, lo cual puede cambiar al realizar el trimming.


## 5.1.9.  Niveles de duplicaci√≥n de secuencias

En este m√≥dulo se grafican los niveles de duplicaci√≥n de secuencias (eje x) contra el porcentaje de secuencias que muestran ese nivel de duplicaci√≥n (eje y), y 

Hay dos l√≠neas en el gr√°fico:
üî¥ l√≠nea roja: Distribuci√≥n para las secuencia de-duplicadas con las proporciones del conjunto de-duplicado las cuales provienen de diferentes niveles de duplicaci√≥n en los datos originales.

üîµ l√≠nea azul: Distribuci√≥n de los niveles de duplicaci√≥n para en conjunto completo de secuencias. 

![niveles de duplicaci√≥n de secuencias en R1 P69](https://user-images.githubusercontent.com/13104654/210295333-f2d9ca7d-0193-4483-b601-e077f178b6c3.png)

La gr√°fica de los niveles de duplicaci√≥n de secuencias muestran en el eje x, el n√∫mero de veces que una secuencia est√° duplicada, y en el eje y el porcentaje de secuencias que muestran ese nivel de duplicaci√≥n. Normalmente un genoma tendr√° un nivel de duplicaci√≥n de secuencias de 1 a 3 para la mayor√≠a de las secuencias, con s√≥lo un pu√±ado de lecturas teniendo un nivel m√°s alto que este; la l√≠nea deber√≠a tener la forma inversa a una gr√°fica log.

En el presente an√°lisis de R1 P69 se no se observan picos a la derecha de la gr√°fica y solo un bajo nivel de duplicaci√≥n al inicio

Un alto porcentaje de duplicaci√≥n de secuencias es un indicativo de contaminaci√≥n.

Este m√≥dulo nos arrojar√° un :warning: **warning** si m√°s del 20% de las secuencias son duplicadas.

Tendr√©mos un :x: **fail** si m√°s del 50% de las secuencias est√°n duplicadas. 

Un warning o fail pueden ser resultado de artefactos de PCR.

#### M√°s acerca de la duplicaci√≥n:

>En una librer√≠a diversa la mayor√≠a de las secuencias se presentar√°n solo una vez en el set final, un bajo nivel de duplicaci√≥n puede indicar un muy alto nivel de coverage de la secuencia blanco, pero un alto nivel puede indicar una clase de sesgo por enriquecimiento ( por ejemplo en la amplificaci√≥n por PCR).
Este m√≥dulo cuenta el grado de duplicaci√≥n para cada secuencia en el conjunto y crea un plot mostrando el numero relativo de secuencias con diferentes grados de duplicaci√≥n.

>Con el fin de reducir los requerimientos de memoria para este m√≥dulo, solamente las secuencias que se presentan en las primeras 200 000 en cada archivo son analizadas, pero esto deber√≠a bastar para obtener una impresi√≥n para los niveles de duplicaci√≥n del archivo completo. 
Cada secuencia es rastreada al final del archivo para dar un conteo representativo del promedio del nivel de duplocaci√≥n. 
Para reducir la cantidad de informaci√≥n en el gr√°fico final, cualquier secuencia con >10 duplicados son colocadas en esta categor√≠a, por lo que no es inusual observar un leve incremento en esta categor√≠a final. Si hay un gran incremento, significa que se tiene un alto n√∫mero de secuencias con alto nivel de duplicaci√≥n. 

>Debido a que la detecci√≥n de la duplicaci√≥n requiere de una coincidencia exacta de secuencias sobre la longitud completa de la secuencia, cualquier lectura por encima de 75pb de longitud son truncadas a 50pb para prop√≥sitos del an√°lisis, a√∫n as√≠, lecturas m√°s largas son m√°s propensas a contener errores de secuenciamiento por lo cual incrementar√° artificialmente la diversidad observada y tender√° a subrepresentar las secuencias altamente duplicadas.

>Para datos del *Whole Genome Shotgun* se espera que cerca del 100% de las lecturas sean √∫nicas (una sola vez en los datos de secuencia). La mayor√≠a de las secuencias deber√≠an caer hacia la izquierda del gr√°fico para ambas l√≠neas. Esto indica una librer√≠a altamente diversa que no esta sobre secuenciada. Si la profundidad del secuenciamento es extremadamente alta (p. ej. >100x el tama√±o del genoma) es inevitable que aparezcan duplicaciones de secuencias: en teor√≠a solo hay un n√∫mero finito de lecturas de secuencia completamente √∫nicas las cuales pueden ser obtenidas de cualquier muestra de DNA ingresada.

>Subconjuntos de enriquecimiento m√°s espec√≠ficos, o la presencia de contaminantes de baja complejidad tender√°n a producir picos hacia la derecha del gr√°fico. Estos picos de altos niveles de duplicaci√≥n aparecer√°n m√°s frecuentemente en la l√≠nea azul ya que conforman una mayor proporci√≥n de la librer√≠a original, pero usualmente desaparecen en el trazo rojo, ya que consiste de una porporci√≥n no significante del conjunto deduplicado. Si los picos persisten en la l√≠nea roja, entonces esto sugiere que hay un alto n√∫mero de secuencias diferentes altamente duplicado lo que podr√≠a indicar ya sea un conjunto de contaminantes o una duplicaci√≥n t√©cnica severa.

>Es usualmente el caso para RNA seq donde existen algunos transcritos altamente abundantes y algunos con baja abundancia. Se espera que las lecturas duplicadas sean observadas para los transcritos de alta abundancia.


## 5.1.10. Secuencias sobre representadas

En el caso de este m√≥dulo:
- Si se calcula que alguna secuencia representa m√°s del 0.1 % del genoma completo ser√° etiquetada como una secuencia sobre-representada y se obtendr√° un :warning: **warning**
- La presencia de secuencias que representan m√°s del 1% del genoma dar√° como resultado un :x: **fail**.

![Sobrerrepresentaci√≥n R1 P69](https://user-images.githubusercontent.com/13104654/210641941-b7fb8d5a-2bce-4183-afa8-bf31b0cf0096.png)

En el presente an√°lisis no se presentaron secuencias sobre-representadas.


> Una librer√≠a normal contendr√° un conjunto diverso de secuencias, ninguna de las cuales individualmente hace una fracci√≥n del completo. Encontrar que una sola secuencia se encuentra sobre representada en el conjunto o significa que es altamente significativa biol√≥gicamente, que la librer√≠a est√° contaminada o bien que no es tan diversa como se esperaba.

> FastQC enlista todas las secuencias que hacen m√°s del 0.1% del total y por cada secuencia busca coincidencias en una base de datos de contaminantes comunes y reportar√° el mejor *Hit*. Los *Hits* deben ser de al menos 20pb en longitud y tener m√°ximo un *mismatch*. Encontar uno no necesariamente significa que sea la fuente de contaminaci√≥n pero puede apuntar en la direcci√≥n correcta. Muchas secuencias de adapadores son muy similares entre s√≠, por lo que podr√≠a tenerse una coincidencia t√©cnicamente incorrecta.

> Los datos de RNAseq pueden tener algunos transcritos que son tan abundantes que se registran como secuencias sobre-representadas. 
Con los datos de DNA seq, ninguna secuencia deber√≠a presentarse con suficientemente alta frecuencia para ser listada, pero algunas ocasiones podemos encontrar un peque√±o porcentaje de lecturas de adaptadores.

> Podemos hacer BLAST de la secuencia sobre representada, si [Blastn](https://blast.ncbi.nlm.nih.gov/Blast.cgi?PROGRAM=blastn&PAGE_TYPE=BlastSearch&LINK_LOC=blasthome) no nos proporciona respuesta, podemos utilizar [VecScreen](https://www.ncbi.nlm.nih.gov/tools/vecscreen/).


## 5.1.11. Contenido de adaptadores 

Este m√≥dulo busca secuencias espec√≠ficas de adaptadores.

   - Una secuencia que representa m√°s del 5% del total causar√° un :warning: **warning** en este m√≥dulo.
   - Una secuencia que represente m√°s del 10% del total causar√° un :x: **fail**.

![Contenido de adaptadores R1 P69](https://user-images.githubusercontent.com/13104654/210295352-5c134059-dc4a-4e72-bee1-18e3ee3eadbb.png)

Nuestro an√°lisis no muestra contaminaci√≥n con secuencias de adaptadores, lo cual es ideal. 
Si existiera un n√∫mero significativo de secuencias de adaptadores, se debe utilizar un programa para recortarlos y realizar el an√°lisis de calidad nuevamente.

Otros gr√°ficos relacionados pueden consultarse en [Documentaci√≥n Contenido de K-mer FastQC](https://www.bioinformatics.babraham.ac.uk/projects/fastqc/Help/3%20Analysis%20Modules/11%20Kmer%20Content.html) y m√°s problem√°ticas en [![QC Fail]](https://sequencing.qcfail.com/).


Lo anterior hay que correrlo para las lecturas R2. Luego podemos utilizar Multiqc para tomar ambos resultados de calidad o bien podemos usar fastp para realizarlo en un solo paso.

## 5.1.12 Multiqc

El siguiente bloque de c√≥digo nos sirve para correr multiqc en colab.

```python
# Pre-alignment Multiqc summary file 
!multiqc .

import multiqc
#El an√°lisis se hace sobre los archivos de fastqc para que te entregue un reporte conteniendo todo
multiqc.run('/content/drive/MyDrive/coÃÅdigos/*_fastqc.zip') #Recordar actualizar la direcci√≥n de donde se encuentren los archivos de salida de qc

```
Para observar el html es con el siguiente bloque:
```python
import IPython

# Best if using Google Colab
IPython.display.HTML(filename='./multiqc_report.html')
#IPython.display.HTML(filename='/content/drive/MyDrive/coÃÅdigos/Secuencias_UADY/multiqc_report.html') #Para direcci√≥n espec√≠fica, marcando error de momento 

```

Es una herramienta que busca todos los archivos de control de calidad de nuestras secuencias y las resume en un solo reporte, adem√°s nos permite subrayar, por ejemplo,
ciertas muestras dentro del reporte, cambiar nombres o bien bajar las gr√°ficas (interactivas) generadas a diferentes resoluciones. 
(es mejor, de momento, tratar de correr esta herramienta en entorno local (de preferencia en linux, unix o wsl) por que no he averiguado como correrla desde colab y que no la coloque en la ventana del cuaderno de trabajo si no que entregue el output externo  ‚ùóüì• ‚ò∫Ô∏è ***Actualizaci√≥n: el c√≥digo funciona adecuadamente y el html tambi√©n se guarda en el entorno del drive.***).
 
# 5.2 Fastp
Fastp es una herramienta que realiza el preprocesamiento y filtrado de calidad de forma paralela y soporta lecturas *Single end* y *Paired end*.
M√°s informaci√≥n se puede encontrar en el [repositorio](https://github.com/OpenGene/fastp#simple-usage) de los desarrolladores.

A comparaci√≥n de FASTQC, fastp ofrece resultados tanto para los datos de prefiltrado como para los datos de post-filtrado, permitiendo una evaluaci√≥n del efecto del filtro comparando directamente las gr√°ficas y reporta sus resultados tanto en formato HTML como en formato JSON, siendo este √∫ltimo manualmente optimizado para facilitar su lectura (m√°s acerca de la descripci√≥n en el [art√≠culo](https://academic.oup.com/bioinformatics/article/34/17/i884/5093234)).
 
Para correr Fastp en los archivos de secuencias (con los datos para filtrado por defecto) dentro de google colab usamos el siguiente bloque de c√≥digo:

```python
# Control de calidad y reporte 
!fastp -i /content/drive/MyDrive/Analisis_Posdoc/PR69/HA1AB3SS04_S4_L1_R1_001.fastq.gz -I /content/drive/MyDrive/Analisis_Posdoc/PR69/HA1AB3SS04_S4_L1_R2_001.fastq.gz -o content/drive/MyDrive/Analisis_Posdoc/PR69/salidas/HA1AB3SS04_S4_L1_R1_001.fastq.gz  -O /content/drive/MyDrive/Analisis_Posdoc/PR69/salidas/HA1AB3SS04_S4_L1_R2_001.fastq.gz

```

## 5.2.1 Summary

En la salida primeramente podemos observar un resumen general antes y despu√©s del filtrado, as√≠ como el resultado del filtro
aplicado, esto lo muestra para los dos archivos de lecturas:

![fastp_summary](https://user-images.githubusercontent.com/13104654/213575454-29eaae55-82c1-4637-b1a5-895cd369057a.png)

En el caso de PR69, con los filtros por defecto, podemos observar que las lecturas de baja calidad contaron para un 3.6% del total, 
sin un porcentaje significativo de N ni lecturas cortas.

## 5.2.2 Adaptadores o mal ligado 

La siguiente secci√≥n muestra las ocurrencias de adaptadores de **ambos** archivos de lecturas.

![adapters_fastp](https://user-images.githubusercontent.com/13104654/213586493-70e52e97-87c2-465c-b510-8b91969bd51b.png)

Para PR69 muestra un bajo porcentaje de adaptadores (0.04% R1 y 0.4% R2). 

En este caso Fastp puede detectar adaptadores y cortarlos lo que nos ahorra tiempo, sin embargo esto se puede realizar con un script aparte utilizando [Trimmomatic](#62-trimmomatic) (o con [cutadapt](#631-cutadapt)) 
para el filtrado de calidad, en este caso, habr√≠a que correr nuevamente los an√°lisis de calidad con Fastqc para ver como quedaron las secuencias.

## 5.2.3 Estimaci√≥n del tama√±o de inserto

En este apartado se muestra la distribuci√≥n del porcentaje de lecturas (eje y) contra el tama√±o de las lecturas (eje x) en un gr√°fico interactivo,
podemos modificar el tama√±o de los ejes y hacer zoom.
Esta estimaci√≥n toma en cuenta el overlap de las lecturas *Paired end*.

En el caso de PR69 se encuentran 56.33% de lecturas no sobrelapadas por lo que podr√≠an ser de tama√±o <30 o >572 o bien con gran cantidad de errores de secuenciaci√≥n.

![Insert Size Distribution](https://user-images.githubusercontent.com/13104654/213606759-021f3824-8b43-470b-917c-17cecc3d64d9.png)

## 5.2.4 Antes del Filtrado
Las secciones siguientes muestran gr√°ficos interactivos, antes del filtrado de calidad,  correspondiendo primero a la **calidad**, 
la gr√°fica es equivalente a la mostrada por fastqc en la secci√≥n 5.1.2 [Calidad de secuencias por base](#512-calidad-de-secuencias-por-base), 

![Calidad_R1](https://user-images.githubusercontent.com/13104654/213797876-82e8a084-a316-42d6-9d73-8d938506a170.png)


luego se muestra el gr√°fico de los √≠ndices del contenido de bases contra la posici√≥n, incluido el contenido de Ns

![Contenido de bases_R1](https://user-images.githubusercontent.com/13104654/213798468-0381814c-c3a1-4790-ab4d-3a4e5edf345e.png)

 y finalmente un heatmap con el conteo de K-meros, 
donde las √°reas m√°s oscuras representan cuentas mayores. Lo anterior primero para R1 y despu√©s para R2.

![Conteo de Kmer](https://user-images.githubusercontent.com/13104654/213798539-eacad1b7-1c3e-4d40-88aa-85b0d4932ea2.png)

La idea de los *K meros* es simple, se crea una ventana de longitud *k* y se desliza tomando un caracter al tiempo. Si la longitud de una secuencia de DNA dada es N, entonces tendremos:

```math
Total K-mer = N - k + 1               
```
Usualmente se buscan tres tipos de frecuencias, la cuenta total (que tantas veces aparece un *k-mero* en una secuencia dada), la cuenta diferida (si ha aparecido o no sin importar cuantas veces) y la cuenta √∫nica (aquellas que solo han aparecido una vez). 
El conteo de *k-meros* es √∫til para el ensamble, clustering y alineamientos, as√≠ como para la correcci√≥n de errores en secuenciado, la estimaci√≥n del tama√±o del genoma y la identificaci√≥n de repeticiones. Computacionalmente es un proceso exigente.

![k-mer](https://user-images.githubusercontent.com/13104654/213822121-8dacf7ca-1e63-4d6d-9094-e6a807141c37.png)


## 5.2.5 Despu√©s del Filtrado
Luego aparece una secci√≥n con gr√°ficos para despu√©s del filtrado (que puede ser por defecto o definido de acuerdo a lo que se observe en los datos).

El html se encuentra en la carpeta de Drive que se indic√≥ en *colab*, y la salida de archivos ya filtrados tambi√©n se encontraran donde se indic√≥, con este archivo fastq podemos continuar con los siguientes an√°lisis *Downstream*. 

fastp tambi√©n cuenta con una *flag* para realizar el *merge* de las dos lecturas. Esto una vez que se cuenta con las secuencias ya filtradas.

# 6. Pre-procesamiento: Filtrado de calidad 

Ahora que se tiene conocimiento acerca de los datos crudos, es importante usar estaa informaci√≥n para limpiar y *Trimmear* las lecturas para mejorar la calidad general antes del ensamble. Hay cierto n√∫mero de herramientas disponibles para esta tarea (a varios grados), pero necesitamos lidiar con lecturas pareadas (en caso de tener lecturas *paired end*, como es el presente caso). Si uno de los *ends* de un par es removido, la lectura hu√©rfana necesita colocarse en un archivo separado de "Lecturas hu√©rfanas", lo cual mantiene el orden de pareado de las lecturas en los archivos para que el programa de ensamble las pueda usar correctamente.
Entre las herramientas m√°s comunes disponibles son Trimmomatic, cutadapt, PRINSEQ, QC Chain entre otras, esta tarea tambi√©n la puede realizar fastp modificando las flags correspondientes.

# 6.1 Fastp (filtrado)

En el caso del filtrado utilizando fastp podemos realizar lo siguiente.
Si se requiere establecer un l√≠mite de longitud para filtrado se utiliza -l, para establecer el nombre de los archivos de salida -j -h, m√°s opciones [aqu√≠](https://github.com/OpenGene/fastp#all-options)

### Longitud m√≠nima de lectura
El valor m√°s apropiado para este par√°metro depender√° de los resultados del reporte de FastQC/Fastp, espec√≠ficamente la longitud de alta calidad en el gr√°fico de la secci√≥n [Per Base Sequence Quality](#512-calidad-de-secuencias-por-base) y segunda y quintas gr√°ficas en la secci√≥n de [Antes del filtrado](#524-antes-del-filtrado) de Fastp.

De nuestros resultados, podemos establecer este m√≠nimo en 36

```python
# Control de calidad y reporte 
#!fastp -i /content/drive/MyDrive/Analisis_Posdoc/PR69/HA1AB3SS04_S4_L1_R1_001.fastq.gz -I /content/drive/MyDrive/Analisis_Posdoc/PR69/HA1AB3SS04_S4_L1_R2_001.fastq.gz -o content/drive/MyDrive/Analisis_Posdoc/PR69/salidas/HA1AB3SS04_S4_L1_R1_001.fastq.gz  -O /content/drive/MyDrive/Analisis_Posdoc/PR69/salidas/HA1AB3SS04_S4_L1_R2_001.fastq.gz -R content/drive/MyDrive/Analisis_Posdoc/PR69/salidas/fastp_report -a --detect_adapter_for_pe

#o tambi√©n de esta forma
# Preprocesamiento 
!fastp -i /content/drive/MyDrive/Analisis_Posdoc/PR69/HA1AB3SS04_S4_L1_R1_001.fastq.gz -I /content/drive/MyDrive/Analisis_Posdoc/PR69/HA1AB3SS04_S4_L1_R2_001.fastq.gz -o content/drive/MyDrive/Analisis_Posdoc/PR69/salidas/HA1AB3SS04_S4_L1_R1_001.fastq.gz  -O /content/drive/MyDrive/Analisis_Posdoc/PR69/salidas/HA1AB3SS04_S4_L1_R2_001.fastq.gz --json="HA1AB3SS04_S4_L1.json" --html="HA1AB3SS04_S4_L1.html" -l 36 --cut_right --cut_front -c -m --merged_out /content/drive/MyDrive/Analisis_Posdoc/PR69/salidas/HA1AB3SS04_S4_L1_mer.fastq.gz --unpaired1 /content/drive/MyDrive/Analisis_Posdoc/PR69/salidas/HA1AB3SS04_S4_L1_up1.fastq.gz --unpaired2 /content/drive/MyDrive/Analisis_Posdoc/PR69/salidas/HA1AB3SS04_S4_L1_up2.fastq.gz --failed_out /content/drive/MyDrive/Analisis_Posdoc/PR69/salidas/HA1AB3SS04_S4_L1_fout.fastq.gz
```
> Podemos usar --detect_adapter_for_pe antes de -- cut right por si hay adaptadores 

> Cut_right equivale a SLIDING WINDOW en Trimmomatic mueve una ventana deslizante desde el frente al final, si encuentra una ventana con calidad media < *Threshold*, se deshace de las bases en la ventana y la parte derecha, luego para, este valor lo podemos establecer a 4:20 (4pb/20 Phred) en Trimmomatic, aqu√≠ dichos valores se establecen por defecto, a menos que especifiquemos lo contrario con otros flags (--cut_right_window_size y --cut_right_mean_quality). 

> Cut_front se mueve del frente 5' a la cola, cortando las bases en la ventana que no alcanzan la calidad media.

> -c activa la correcci√≥n de bases en regiones sobrelapadas (solo para lecturas PE). 

> -m para inputs *paired end*, combina cada par de lecturas en una sola si se encuentran sobrelapadas. Las lecturas que se combinan ser√°n escritas en el archivo dado por --merged_out, las lecturas sin combinar se especifican en --out1 y --out2. El modo combinado por defecto se encuentra desactivado.

> En el caso de unpaired1 y unpaired2 para PE, si la lectura1 pasa QC pero la lectura2 no ser√° escrita en unpaired1, viceversa para unpaired2. Si unpaired1 y unpaired2 son la misma, ambas ser√°n escritas en el mismo archivo.

>  en el flag de Failed_out se escriben las lecturas que no pasan los filtros.

CH606 Fastp bloque

```python

!fastp -i fwd.fq.gz -I Reverse.fq.gz -o fwdR1_filtrada.fq.gz  -O RvR2_filtrada.fastq.gz -l 100 --cut_right --cut_front -c -D --dedup -m --merged_out salida_merge.fq.gz --unpaired1 up1.fq.gz --unpaired2 up2.fq.gz --failed_out fout.fq.gz

```

Ejemplos 
```python
!fastp -i /content/drive/MyDrive/PR69/HA1AB3SS04_S4_L1_R1_001.fastq.gz -I /content/drive/MyDrive/PR69/HA1AB3SS04_S4_L1_R2_001.fastq.gz -o /content/drive/MyDrive/PR69/Salidas/FastPR1filtrado.fastq.gz -O /content/drive/MyDrive/PR69/Salidas/FastPR2filtrado.fastq.gz -l 36 --cut_right --cut_front -c -m --merged_out /content/drive/MyDrive/PR69/Salidas/mergedPR69.fastq.gz --unpaired1 /content/drive/MyDrive/PR69/Salidas/Up1PR69.fastq.gz --unpaired2 /content/drive/MyDrive/PR69/Salidas/Up2PR69.fastq.gz --failed_out /content/drive/MyDrive/PR69/Salidas/failedPR69

##Filtrado desduplicado
!fastp -i /content/drive/MyDrive/PR69/Salidas/FastPR1filtrado.fastq.gz -I /content/drive/MyDrive/PR69/Salidas/FastPR2filtrado.fastq.gz -o /content/drive/MyDrive/PR69/Salidas/FastPR1filtradodesduplicado.fastq.gz -O /content/drive/MyDrive/PR69/Salidas/FastPR2filtradodesduplicado.fastq.gz -D --dedup


!fastp -i /content/drive/MyDrive/PR69/Salidas/filtered_tile1_1.fastq -I /content/drive/MyDrive/PR69/Salidas/filtered_tile2_2.fastq -o /content/drive/MyDrive/PR69/Salidas/FastPR1filtrado_bbmap.fastq.gz -O /content/drive/MyDrive/PR69/Salidas/FastPR2filtrado_bbmap.fastq.gz -l 40 --length_limit 280 --cut_right --cut_front -c -y -p -m --merged_out /content/drive/MyDrive/PR69/Salidas/mergedPR69_bbmap.fastq.gz --unpaired1 /content/drive/MyDrive/PR69/Salidas/Up1PR69_bbmap.fastq.gz --unpaired2 /content/drive/MyDrive/PR69/Salidas/Up2PR69_bbmap.fastq.gz --failed_out /content/drive/MyDrive/PR69/Salidas/failedPR69_bbmap

```


# 6.2 Trimmomatic

Para el filtrado y corte de adaptadores con trimommatic, se puede usar el siguiente bloque de c√≥digo 
```python
# Trimming 
!trimmomatic PE -phred33 R1.fastq R2.fastq R1\_paired.fq.gz R1\_unpaired.fq.gz R2\_paired.fq.gz R2\_unpaired.fq.gz ILLUMINACLIP:contams\_forward\_rev.fa:2:30:10 LEADING:3 TRAILING:3 SLIDINGWINDOW:4:15 MINLEN:36
```

o bien podemos se√±alarle la secuencia de adaptadores. En el caso de PR69, las secuencias no tienen adaptador (o eso creemos) y tienen la misma longitud. 
En el presente caso no conozco la secuencia de adaptadores de la plataforma pero si se requiere, aqui podemos encontrar algunos adaptadores [https://github.com/ATGenomics/adapters](https://github.com/ATGenomics/adapters)

El siguiente bloque de c√≥digo se aplica en una m√°quina local y no en google colab como hemos utilizado hasta ahora, podr√≠a funcionar si llamamos el shell y le indicamos alguna ruta espec√≠fica para clonar las secuencias del repositorio. 

*Para secuencias Paired End:

```shell
cd $HOME/ git clone https://github.com/ATGenomics/adapters.git $HOME/bin/adapters

adapters="$HOME/bin/adapters/NexteraPE-PE.fa"
```

adapt√°ndolo para colab podr√≠a ser de la siguiente manera:

```shell
%%shell

###Para no poner toda la direcci√≥n asignamos una variable, probar si esto funciona
#### Asignar nombres de archivos de lecturas
#fwd="/content/drive/MyDrive/Analisis_Posdoc/PR69/HA1AB3SS04_S4_L1_R1_001.fastq.gz"
#rev="/content/drive/MyDrive/Analisis_Posdoc/PR69/HA1AB3SS04_S4_L1_R2_001.fastq.gz"
#name="PR69"

#Primero creamos el directorio adaptadores
mkdir -p /content/drive/MyDrive/Analisis_Posdoc/PR69/adapters

#luego clonamos el repositorio dentro de esta carpeta
cd /content/drive/MyDrive/Analisis_Posdoc/PR69/adapters git clone https://github.com/ATGenomics/adapters.git $HOME/bin/adapters

#una vez clonado se define como una variable a la secuencia de adaptadores que corresponda la plataforma que estemos usando
adapters="/content/drive/MyDrive/Analisis_Posdoc/PR69/adapters/NexteraPE-PE.fa"
```

En una m√°quina local se activa el entorno qc 
`source activate qc`

Sin embargo en google colab esto no parece ser necesario puesto que simplemente se llama el shell o bash con la correspondiente funci√≥n (me falta investigar m√°s a fondo este aspecto). 


```shell
%%shell
trimmomatic PE -phred33 -threads 16 \ fwd \ rev  \ /content/drive/MyDrive/Analisis_Posdoc/PR69/HA1AB3SS04_S4_L1_R1_001.trim.fq.gz \ /content/drive/MyDrive/Analisis_Posdoc/PR69/HA1AB3SS04_S4_L1_R2_001.trim.fastq.gz /Analisis_Posdoc/PR69/HA1AB3SS04_S4.trim.fq.gz \ ILLUMINACLIP:${adapters}:2:30:10 SLIDINGWINDOW:4:20 MINLEN:90 CROP:150
```
Es muy importante se√±alar que en este caso hay que tomar en cuenta el orden en el que se colocan los par√°metros.

>ILLUMINACLIP:${adapters} eliminamos adaptadores con cierta frecuencia (2:30:10)

>SLIDINGWINDOW: 4:20 cuatro nucle√≥tidos en promedio tienen una calidad menor a 20 se elimina la secuencia (incluyendo el par).

>MINLEN: m√≠nimo de largo 90

>CROP: Cortar la lectura a una longitud de 150


Trimmomatic realiza un *trimming* de calidad adaptativo, cortado de cabeza y cola y remoci√≥n de adaptadores. Se puede revisar la documentaci√≥n y bajar el programa [aqu√≠](http://www.usadellab.org/cms/index.php?page=trimmomatic).

Una de las ventajas del programa es que permite trabajar con secuencias *Paired end*, reteniendo solamente pares coincidentes.
Otra ventaja es que permite coincidencias parciales y *overlapping* para la b√∫squeda de adaptadores.

Las opciones que podemos utilizar son las siguientes:

## 6.2.1 Eficiencia y formato

>Las siguientes se usan siempre antes de la invocaci√≥n de los archivos de entrada y salida

- *threads*: este ajuste modifica el n√∫mero de "hilos" de CPU que Trimmomatic deber√≠a usar en las computaciones. Una computadora t√≠picamente tiene cerca de 2 n√∫cleos. los cuales deber√≠an corresponder a una cantidad de 4 hilos disponibles. 
- *phred*:  [-phred33 	-phred64]:  Este ajuste le dice al programa que codifique el archivo

> A partir de aqu√≠ son las opciones que van despu√©s de la invocaci√≥n de *Inputs/Outputs* (estas opciones se presentan en may√∫sculas) 

Opciones para cambiar la codificaci√≥n (ver en [Apartado 5](#5-pre-procesamiento-analisis-de-calidad-usando-fastqc-y-fastp)):
Si se requiere leer la codificaci√≥n de un tipo y sacar la codificaci√≥n de uno diferente, √©stas opciones son las que se necesitan utilizar.

- TOPHRED33: Convierte *scores* de calidad a Phred-33
- TOPHRED64: Convierte *scores* de calidad a Phred-64 

### 6.2.1.1 Cortado (*Cropping*)

Trimmomatic cuenta con varias opciones que pueden ser usadas simult√°neamente o no:

-LEADING: Corta bases del inicio de una lectura, si est√° por debajo del umbral de calidad - adaptativa
-TRAILING: Corta bases del final de una lectura, si est√° por debajo del umbral de calidad - adaptativa
-CROP: Corta la lectura a una longitud espec√≠fica
-HEADCROP: Corta el n√∫mero espec√≠ficado de bases del inicio de una lectura.

LEADING y TRAILING son cortado adaptativo, lo que significa que cortar√°n el inicio/fin de las lecturas si fallan la calidad especificada. Lo anterior difiere de CROP y HEADCROP, los cuales podr√≠an cortar a una longitud o n√∫mero de bases espec√≠ficas (respectivamente), en este caso, el programa realizara el corte para todas las lecturas.

-MINLEN: se deshar√° de todas las lecturas que caen bajo una longitud especificada.

### 6.2.1.2 *Trimming* de calidad adaptativo

-SLIDINGWINDOW: realiza un trimming en una ventana de deslizamiento, cortando una vez que la calidad promedio caiga de un umbral especificado.
Toma dos valores como `SLIDINGWINDOW:4:15` lo que significa "Escanear la lectura con una amplitud de ventana de 4 bases, cortando cuando la calidad promedio por base caiga debajo de 5'   

### 6.2.1.3 *Trimming* de adaptadores

Finalmente, trimmomatic tomar√° un archivo con las secuencias de los adaptadores y las cortar√°. Siguiendo por ejemplo la llamada: `ILLUMINACLIP:<fastaWithAdaptersEtc>:<seed mismatches>:<palindrome clip treshold>:<simple clip treshold>` d√≥nde:

-fastaWithAdaptersEtc: Especifica el *path* a un archivo fasta conteniendo todos los adaptadores, secuencias PCR etc. El nombre de las diferentes secuencias dentro de este archivo determina como ser√°n usadas.
-seedMismatches: Especifica la cuenta m√°xima de *mismatches*, lo cu√°l podr√≠a seguir permitiendo una coincidencia completa.
-palindromeClipTreshold: especifica que tan precisa es la coincidencia entre las dos lecturas 'ligadas por adaptador' que deben ser pal√≠ndromo para las lecturas *PE*
-simpleClipTreshold: especifica que tan precisa debe ser la coincidencia entre cualquier adaptador, etc contra una lectura.

  ![trimmomatic_adapter](https://user-images.githubusercontent.com/13104654/211375856-b34becba-e0e4-450d-8d0b-06552b13b296.png)

Existen 4 posibles escenarios que Trimmomatic puede cubrir:
A. Una secuencia t√©cnica es completamente cubierta por la lectura y as√≠, un alineamiento simple podr√° identificarla.
B. Solamente existe una coincidencia parcial entre la secuencia t√©cnica y la lectura y as√≠, es necesariio un alineamiento corto.
C. y D. Ambos pares son probados a la vez, permitiendo que suceda lo siguiente: "es mucho m√°s confiable que un alineamiento corto (B.) y permite que se detecte la lectura del adaptador incluso cuando solo se ha secuenciado una base de este."

El umbral de clip palindr√≥mico escencialmente dice que tan preciso debe ser el alineamiento de adaptadores. Esto es la probabilidad log10 de obtener una coincidencia por una posibilidad aleatoria, y as√≠, los valores alrededor de 30 son recomendados.

Referencias: https://jshleap.github.io/bioinformatics/writting-jNGS_tutorial/#encoding
[Bolger *et al.*, 2014](https://academic.oup.com/bioinformatics/article/30/15/2114/2390096)

# 6.3 Otras herramientas
## 6.3.1 Cutadapt

Cutadapt busca el adaptador en todas las lecturas y lo remueve cuando lo encuentra. A menos que se use la opci√≥n de filtrado, todas las lecturas que est√°n presentes en el archivo *input* estar√°n presentes en el *output*, algunas de ellas ya con trimm, otras no. Incluso las lecturas que fueron coradas a una longitud de 0 son un output. Esto puede ser modificado en el comando (opciones).
Puede detectar m√∫ltiples tipos de adaptadores. Adaptadores 5' preceden la secuencia de inter√©s mientras que los 3' la siguen. Las distinciones se hacen dependiendo de donde se presenta la secuencia en la lectura. Adem√°s tambi√©n permite el procesamiento de lecturas *Paired End*.


![imagen](https://user-images.githubusercontent.com/13104654/212785938-7cfd92e4-acfd-4e47-86af-cc90123776c3.png)


Con lo anterior en mente, es una herramienta vers√°til para remover *primers* o en general oligos de las regiones que flanquean el DNA. La gu√≠a de uso se puede encontrar en su [p√°gina](https://cutadapt.readthedocs.io/en/stable/guide.html).

con el siguiente bloque de c√≥digo se puede utilizar

```bash

cutadapt -a ADAPTER_FWD -A ADAPTER_REV -o out.1.fastq -p out.2.fastq reads.1.fastq reads.2.fastq

```
Referencia: https://jshleap.github.io/bioinformatics/writting-jNGS_tutorial/#encoding

## 6.3.2 Seqkit

[Seqkit](https://bioinf.shenwei.me/seqkit/) es un programa o l√≠nea de comandos, que permite, no solo eliminar duplicados, si no tambi√©n manipular secuencias (solamente en formato fastq) eficientemente. El c√≥digo fuente se puede encontrar [aqu√≠](https://github.com/shenwei356/seqkit).  

> (formatos BAM y SAM tienen que ser convertidos a fastq antes de utilizarse con seqkit).

El siguiente bloque de c√≥digo funciona localmente para eliminar duplicados

```Bash
seqkit rmdup -s -o clean.fastq input.fastq 
#Remueve secuencias duplicadas del input.fastq y las guarda en clean.fastq 
```

Posiblemente en google colab
```Bash
!seqkit rmdup -s -o ./sec_limpias.fastq ./archivo_salida_dePreproc/PR69.fastq
```
> Se tendr√≠a que tomar como input el archivo de salida del √∫ltimo paso del preprocesamiento, que podr√≠a ser el de salida de cutadap.

## 6.3.3 Trim-Galore
Para instalar se puede realizar con conda
```bash
conda install -c bioconda trim-galore
```


:alien: üëΩ :alien: :alien: :alien: :alien: :alien: :alien: :alien: :alien: :alien: :alien: :alien: :alien: :alien: :alien: :alien: :alien: :alien: :alien: :alien: :alien: :alien: :alien: :alien: :alien: :alien: :alien: :alien: :alien: :alien: :alien: :alien: :alien: :alien: :alien: :alien: :alien: :alien: :alien: :alien:

## 6.4 N√∫mero y longitud de secuencias despu√©s del filtrado de calidad

Una vez realizado el filtrado se pueden correr de nuevo los an√°lisis de calidad (usando fastqc/multiqc o fastp). Adem√°s podemos utilizar nuevamente los comandos de bash para analizar longitudes de lecturas, etc.

El bloque de c√≥digo para estas revisiones (realizando el trimming con trimmomatic) ser√≠a el siguiente:

> (Tambi√©n revisamos que se encuentren pareados, esta parte se puede omitir)

El siguiente bloque permite revisar si los archivos est√°n pareados
```bash
##Para explorar que los archivos est√°n pareados
%%bash
zcat /content/drive/MyDrive/Analisis_Posdoc/PR69/HA1AB3SS04_S4_L1_R1_001_Trim.fastq.gz | wc -l
zcat /content/drive/MyDrive/Analisis_Posdoc/PR69/HA1AB3SS04_S4_L1_R2_001_Trim.fastq.gz | wc -l
```
si queremos saber cual es el n√∫mero de secuencias usamos el siguiente bloque de c√≥digo, de nuevo usamos zgrep por que es un archivo comprimido y ahora usaremos los archivos de salida del trimming (o + cutadapt + seqkit).

```bash
##revisar nuevamente la cantidad de secuencias
%%bash
zgrep '^@M02521' /content/drive/MyDrive/Analisis_Posdoc/PR69/HA1AB3SS04_S4_L1_R1_001_Trim.fastq.gz | wc -l
zgrep '^@M02521' /content/drive/MyDrive/Analisis_Posdoc/PR69/HA1AB3SS04_S4_L1_R2_001_Trim.fastq.gz | wc -l
```

Exploramos la longitud de secuencias. Para ello podemos usar awk de nuevo.

Para cada l√≠nea de secuencia podemos contar cada caracter usando el par√°metro NR (n√∫mero de registros) y usando el contador y a√±adiendo para imprimir en txt.

```bash
%%bash
zcat /content/drive/MyDrive/Analisis_Posdoc/PR69/HA1AB3SS04_S4_L1_R1_001_Trim.fastq.gz | awk 'NR%4 == 2 {lengths[length($0)]++ ; counter++} END {for (l in lengths){print l, lengths[l]}}' | sort -n | uniq -c > /content/drive/MyDrive/Analisis_Posdoc/read_length1Trim.txt

zcat /content/drive/MyDrive/Analisis_Posdoc/PR69/HA1AB3SS04_S4_L1_R2_001_Trim.fastq.gz | awk 'NR%4 == 2 {lengths[length($0)]++ ; counter++} END {for (l in lengths){print l, lengths[l]}}' | sort -n | uniq -c > /content/drive/MyDrive/Analisis_Posdoc/read_length2Trim.txt


```

el txt generado lo podemos importar y transformar nuevamente a csv para despu√©s graficarlo en matplotlib, se aplican los siguientes bloques:

```python
read_file = pd.read_csv (r'/content/drive/MyDrive/Analisis_Posdoc/read_length1Trim.txt',header=None)
read_file.to_csv (r'/content/drive/MyDrive/Analisis_Posdoc/read_lengthR1Trim.csv', index=None,header=None)

read_file = pd.read_csv (r'/content/drive/MyDrive/Analisis_Posdoc/read_length2Trim.txt',header=None)
read_file.to_csv (r'/content/drive/MyDrive/Analisis_Posdoc/read_lengthR2Trim.csv', index=None,header=None)

```

## 6.5 PhiX 

> Es posible usar esta librer√≠a y aplicar este c√≥digo, sin embargo, en este caso no se hizo una corrida por lo que este c√≥digo queda en stand by y se podr√≠a usar para futuras secuenciaciones (podemos usar el c√≥digo para evitar remanentes de un control interno de la secuenciaci√≥n)

Para control de calidad interno 
[Phix](https://support.illumina.com/bulletins/2017/02/what-is-the-phix-control-v3-library-and-what-is-its-function-in-.html) o la librer√≠a de control v3 PhiX (FC-110-3001) es derivada del genoma del bacteri√≥fago, bien caracterizado, PhiX. Es una librer√≠a concentrada de Illumina (10 nM en 10 ¬µl) que tiene un tama√±o promedio de 500 pb y cuenta con una composici√≥n balanceada de bases a ~45% GC y ~55% AT.

Puede servir como un control de calibraci√≥n y puede ser secuenciado solo,  para usarlo en: C√°lculos de phasing y prephasing, generaci√≥n de matrices Cross talk (ref cruzada) y examinar el desempe√±o promedio de las plataformas de secuenciaci√≥n Illumina. 

Se puede bajar un programa [BWA](https://github.com/lh3/bwa#type), para realizar el index de la referencia.
BWA: es un paquete de software para el mapeo de secuencias de ADN contra un gran genoma de referencia, como el genoma humano. A mayor n√∫mero de lecturas de Phix, mayor probabilidad de tener un Profago en el genoma problema. Evidentemente me quedar√≠a con las secuencias que no mapearon vs PhiX.

El siguiente bloque de c√≥digo es SOLAMENTE para aplicarse en el entorno local de una m√°quina (no en colab)

```bash
#qc corresponde en este caso a la carpeta de trabajo donde se encuentran todas las secuencias en el presente bloque de c√≥digo, depender√° de cada  persona y m√°quina donde se trabaje (es el nombre que cada quien decide)

cd $HOME/Ensamble #Este comando va a corresponder con la carpeta en la que nos encontremos trabajando localmente, en el caso de colab esto no se realiza 
source activate qc #En este comando estamos activando la carpeta qc como la fuente para trabajar
tar -zxvf PhiX_Illumina_RTA.tar.gz && rm PhiX_Illumina_RTA.tar.gz # hay que definir el directorio
bwa index PhiX/Illumina/RTA/Sequence/Chromosomes/phix.fa -p ¬¥01_qc/phix¬¥ # Este es el comando que corresponde al programa BWA para hacer el index
conda deactivate
```
Para colab, primeramente hay que tener considerada la [instalaci√≥n](#instalacion-de-bwa-y-samtools) y definir el [directorio de las secuencias de la librer√≠a Phix](https://drive.google.com/drive/folders/1FqceccL4vIdQRCtta0eGVS_NZiBX55Kz). Podr√≠amos usar el siguiente bloque:

```python
!bwa index /content/drive/MyDrive/Analisis_Posdoc/PhiX/Illumina/RTA/Sequence/Chromosomes/phix.fa -p *./calidad1/phix

```


El siguiente bloque (local) se utiliza para Mapear (por Alineamiento) las *lecturas* versus la referencia, en este caso PhiX:
Consta de tres pasos

1. script usando | Indexar y alinear vs esa referencia
2. Selecciona las secuencias que no mapean vs PhiX
3. Sort=ordenar esas secuencias no mapeadas


```bash 
#De igual forma primero nos movemos a la carpeta ensamble y activamos el entorno del archivo qc
cd $HOME/Ensamble 
source activate qc

#busca el alineamiento, selecciona las secuencias que no mapean y se ordenan dichas secuencias 

bwa mem -t 8 01_qc/phix \
    01_qc/R1.trim.fastq.gz \
    01_qc/R2.trim.fastq.gz | \
    samtools view -bS -f4 - | \
    samtools sort -@ 4 - -o 01_qc/PB69.phix.sorted.bam

samtools fastq \
    -1 01_qc/R1.trim.clean.fastq.gz \
    -2 01_qc/R2.trim.clean.fastq.gz \
    -s 01_qc/S.trim.clean.fastq.gz \
    01_qc/PB69.phix.sorted.bam
```

```bash
ls -ltrh 01_qc/
zcat *./R1.trim.clean.fastq.gz | awk 'END{ print NR/4 }'
zcat *./R2.trim.clean.fastq.gz | awk 'END{ print NR/4 }'
```

El bloque en colab (9min) corresponder√≠a a:

```python 

#busca el alineamiento, selecciona las secuencias que no mapean y se ordenan dichas secuencias 
#estableceremos threads en 2 (colab utiliza al menos 2 cores)

!bwa mem -t 2 *./calidad1/phix \
    ./R1.filtrado.fastq.gz \
    ./R2.filtrado.fastq.gz | \
    samtools view -bS -f4 - | \
    samtools sort -@ 2 - -o salidas/PR69.phix.sorted.bam

!samtools fastq \
    -1 salidas/R1.filtradas.clean.fastq.gz \
    -2 salidas/R2.filtradas.clean.fastq.gz \
    -s salidas/S.filtradas.clean.fastq.gz \
    salidas/PR69.phix.sorted.bam
```
> index sirve para indexar la base de datos de la secuencia con el flag -p que define la base de datos de salida

> mem es el algoritmo (Maximal Exact Matches) para el alineamiento contra la referencia, las flags: -t define el n√∫mero de hilos a usar, luego hay que incluir los archivos del mapeo (fwd y reverse, es importante tomar las secuencias ya filtradas), samtools nos sirve para interactuar con las secuencias, seleccionando y ordenando aquellas que no mapean contra phix y las escribe en el archivo (-o) sorted.bam (cambia el formato), comparando  bam (-b) contra fa (-f), y ordenando (sort) usando 4 hilos (-@). 

> fastq transforma las secuencias bam no mapeadas a fastq en archivos espec√≠ficos (-1 y -2), as√≠ como definir las salidas de lecturas de singletones (-s)

Se puede realizar el Conteo de las secuencias, NUEVAMENTE.
.clean son archivos de salida despu√©s de mapear vs PHIX (nosotros establecemos el nombre)

![BAM formato](https://user-images.githubusercontent.com/13104654/216114543-cd5f515e-eb53-4024-9d96-9e2421b34902.png)

```bash
%%bash
zcat R1.trim.clean.fastq.gz | awk 'END{ print NR/4 }'
zcat R2.trim.clean.fastq.gz | awk 'END{ print NR/4 }'
```
Si no se encuentran secuencias de Phix entonces se puede realizar el ensamble con las secuencias filtradas (trimmeadas), de lo contrario se usar√°n las establecidas en -1 y -2.

https://github.com/Microfred/IntroBioinfo/blob/main/Unidad_3/Readme.md


## 6.6 Determinacion de distribucion de *K-meros*

Aunque hoy en d√≠a los ensambladores pueden trabajar distintos tama√±os de k-meros, hay que tener en cuenta que elegir un tama√±o inapropiado pordr√≠a afectar enormemente la calidad de un ensamble. Para ello podemos realizar un escaneo antes, tal como lo vimos en el [apartado 5.2.4 Antes del Filtrado](#524-antes-del-filtrado) de Fastp, este realiza y proporciona un heatmap con un conteo de distribuci√≥n de 5-meros, lo que en primera instancia nos podr√≠a servir para tener una idea de la distribuci√≥n de *k-meros*. 

(contar k meros, para modificar ciertos par√°metros para el ensamble)

Existen otras herramientas que podemos usar  para esta tarea, entre ellas:

### 6.6.1 Kmergenie
[kmergenie](http://kmergenie.bx.psu.edu/) estima la mejor longitud de *K-mero* para en ensamble *De Novo*. Dado un conjunto de lecturas, KmerGenie primero c√°lcula un histograma de abundancias para muchos valores de *K*. Entonces, para cada valor de *k*, predice el n√∫mero de distintos *k-meros* gen√≥micos en el dataset y entrega la longitud que maximiza este n√∫mero. Las precicciones pueden ser aplicadas a ensambles single-k (Velvet, SOAPdenovo 2, ABySS, Minia). Sin embargo ensambladores multi-k se desempe√±an mejor, generalmente, con los par√°metros por defecto, usando m√∫ltiples, m√°s que solo el mejor k predicho. 

El bloque de c√≥digo de kmergenie en colab, ser√° dif√≠cil de aplicar puesto que utiliza instancias de R, lo cual no s√© si dificulte la computaci√≥n. Es posible llamar directamente el repositorio de github para que use esas dependencias. 

> se puede aplicar el ensamble directamente con SPAdes o megahit con los valores por defecto.

### 6.6.2 Velvetadvisor

[Velvet advisor](https://dna.med.monash.edu/~torsten/velvet_advisor/) es una herramienta de c√°lculo de K-meros, en la cu√°l, basado en el total de lecturas arroja el valor de K que podr√≠a funcionar para el ensamble.
En el caso de PR69 este valor podr√≠a ser 287 con 20 veces el *k-mer* coverage para mi ensamble (sugiere entre 10 y 30) 
En el caso de CH606 podr√≠a ser de 147 con 20 veces el *k-mer* coverage para mi ensamble (sugiere entre 10 y 30) 

Todos los valores de cobertura en Velvet son proporcionados en cobertura de *k-mer*, por ejemplo que tantas veces tiene que ser visto un *k-mero* entre las lecturas. El √≠ndice entre la cobertura de *k-mer* (Ck) y la cobertura est√°ndar (amplitud de nucle√≥tido) (C) es: 

```math
Ck = C * (L - k + 1) / L
```
D√≥nde `k` es la longitud de hash (pica, trocea, mezcla) y `L` la longitud de la lectura.
La elecci√≥n de k es un intercambio entre especificidad y sensibilidad. La experiencia muestra que Ck deber√≠a encontrarse por encima de 10 para empezar a tener buenos resultados, arriba de 20 se podr√≠a estar desperdiciando cobertura. Adem√°s, tambi√©n por experiencia, las pruebas emp√≠ricas con diferentes valores de k no son costosas de correr.

![velvet advisor ej](https://user-images.githubusercontent.com/13104654/217328508-8cc231bd-7e81-4355-9312-e773e6278f70.png)

o bien correr las opciones velvetk y [velvet optimizer](https://github.com/tseemann/VelvetOptimiser) para elegir el mejor set.

Adem√°s, en caso de utilizar velvet, advisor recomienda usar `-exp_cov auto ` y `-cov_cutoff auto ` en velvetg cuando se exploren los datos por primera vez. 

# 7. Ensamble *De Novo*

Una vez se ha evaluado el control de calidad de las secuencias, √©stas se encuentran mezcladas y, como si de un rompecabezas se tratara, hay que armarlo en el orden correcto. El m√©todo de alineamiento utilizado para realizar esta tarea se denomina ensamble. Una parte esencial del ensamble es el alineamiento, que involucra disponer de una cantidad masiva de lecturas de DNA, buscando regiones que coincidan unas con otras (regiones de alineamiento) y eventualmente unir el rompecabezas.

Para el Ensamble *De novo*, dependiendo de la plataforma de secuenciaci√≥n es posible aplicar diferentes estrategias de ensamble:

![4 estrategias de ensamble](https://user-images.githubusercontent.com/13104654/212998923-3620318d-7258-49da-838f-3e63274b195f.png)

[Estrategias de ensamble *De Novo* en secuencias de lectura corta](https://academic.oup.com/bib/article/11/5/457/1746253?login=true)

Los algoritmos de ensamble son la colecci√≥n de procesos para construir, a partir de cantidades de lecturas de secuencias cortas,  secuencias de DNA original. Las secuencias son alineadas unas con otras y las partes que se superponen son combinadas en una secuencia estrecha. Actualmente, existen dos m√©todos de algoritmos de ensamble, que diferir√°n de acuerdo a la complejidad de los datos de secuenciaci√≥n. 

# 7.1 M√©todos
## 7.1.1 OLC (Overlap Layout Consensus) - Ensambladores gr√°ficos de caracteres

Este algoritmo reconoce intersectos entre combinaciones de lecturas para construir una gr√†fica de las conexiones entre las Lecturas de secuenciaci√≥n. 
Es una aproximaci√≥n computacionalmente intensiva donde la complejidad de la computaci√≥n incrementa con el total de los datos de secuenciaci√≥n usados durante el ensamblaje. Debido a lo cual este algoritmo se vuelve inexcusable con los secuenciadores como Ilumina, donde millones de lecturas de secuencia corta son necesarias para el ensamble. 

Despu√©s de generado el gr√°fico, visita cada nodo usando un m√©todo de la teor√≠a de grafos llamado camino Hamiltoniano, para construir el ensamble final.
La etapa de dise√±o del algoritmo reduce la complejidad del gr√°fico preliminar, por condensaci√≥n de las √°reas que surgen sin amig√ºedad del mismo loci gen√≥mico al nodo donde la l√≠nea diverge con varios caminos potenciales. 
Es bastante dif√≠cil la selecci√≥n de ese camino. Esto arroja subgrafos para hacer *contigs* que se pueden describir como secuencias *unitig* ensambladas inequ√≠vocamente que tienen una alta profundidad de secuenciaci√≥n y est√°n vinculadas a un gran n√∫mero de otros *contigs*. Ahora, el unitig se empareja con otros unitigs para formar una secuencia de andamios (*Scaffolds*).

El √∫ltimo paso para realizar el proceso de **consenso** incluye la lectura a trav√©s de subgrafos contiguos y extraer la secuencia de consenso para lecturas de cada subgrafo. Otro algoritmo de caracteres involucra la misma teor√≠a de sobrelape de grafos, pero difiere ligeramente ya que simplifica el gr√°fico removiendo bordes transitivos que tienen detalles redundantes.

Referencias: Li et al., 2012; Chang et al. 2012

![Hamiltonian Path y Consenso](https://user-images.githubusercontent.com/13104654/215958857-f462cfc2-9155-494b-ac6a-13cf6e8d525a.png)
Commins *et al.*, 2009  

![Layout1](https://user-images.githubusercontent.com/13104654/216516419-c8c0b76a-8eae-45cc-ae15-78e1038a0e28.png)

![Layout2](https://user-images.githubusercontent.com/13104654/216521652-b3a7185c-baff-45c0-8531-add7d2a25cb6.png)

![Layout3](https://user-images.githubusercontent.com/13104654/216521784-3320bf35-4111-40f9-9b08-e063bb5363c2.png)

![Consenso](https://user-images.githubusercontent.com/13104654/216521977-9021f8ac-a19a-4ec4-9e85-ff382baa3324.png)


## 7.1.2 Gr√°ficos De Brujin 

Esencialmente, los ensambladores con gr√°ficas De Brujin, rompen las lecturas en subsecuencias de longitud-*k* (*k-meros*), us√°ndolos posteriormente para construir una gr√°fica. Los nodos de la gr√°fica representan *k-meros* en este caso, los bordes indican los *k-meros* vecinos que sobrelapan con k-1 base. Las lecturas no se representan como tal si no que se representan por paths. Esta estructura esta basada en la identidad precisa entre los *k-meros*. En la gr√°fica, los caminos divergentes son generados por errores de secuenciaci√≥n que reducen la longitud de los *paths*. Los ensambladores usualmente monitorean la cobertura k-mer de cada nodo, lo que permite que el gr√°fico sea
limpiado eliminando las puntas de baja cobertura. Por lo general, los ensambladores basados en gr√°ficos De Brujin consumen mucha memoria, aunque se utilizan otros m√©todos para eficientarlo.

![De Brujin](https://user-images.githubusercontent.com/13104654/216419942-5f37e8ee-c7ea-4fa9-a346-a7b9bdaf6811.png)
Ayling *et al.*, 2019

Existen algunas limitaciones inherentes al ensamble con dBg, como la selecci√≥n inicial del tama√±o del *k-mero* con el cu√°l se construir√° la gr√°fica. Elegir un tama√±o inapropiado pordr√≠a afectar enormemente la calidad de un ensamble. Peque√±os *k-meros* llevan a gr√°ficas m√°s conectadas; los m√°s largos proporcionan mayor especificidad y pocos loops, pero m√°s desconectados como resultado de los gaps o errores dentro de los datos de las lecturas o la falta de cobertura del genoma. Una limitaci√≥n adicional es que el el tama√±o del *k-mero* no puede exceder pr√°cticamente dos menos que la longitud de lectura para generar al menos dos bordes.

![Repeats_ assembly](https://user-images.githubusercontent.com/13104654/216512545-b062af0b-e4e9-4ebd-836f-57ca4ff8974d.png)


![dBGexample](https://user-images.githubusercontent.com/13104654/216523981-95ea24d8-b5a9-4cab-a7f9-1f8256287c0e.png)

![dBGErrorCorrect](https://user-images.githubusercontent.com/13104654/216524134-cacb944c-5fd1-4a42-920c-fea52911c0ce.png)

# 7.2 Ensambladores
> **Resumen de ensambladores en colab**: Los siguientes ensambladores pueden aplicarse en el entorno de colaboratory:  ‚úÖMegahit se aplica sin problemas, ‚úÖSPAdes se aplica sin problemas en colab, ‚òëÔ∏èALGA se realiz√≥ solo en WSL, ‚ùåUnicycler no ha funcionado en Colab ni en WSL (probablemente por que usa instancias de SPAdes y no lo puede llamar desde el path de colab, falta revisar como se puede definir el path); ‚úÖAbyss se instal√≥ con conda y se realiz√≥ el ensamble sin problema, ‚úÖvelvet se pudo instalar con conda pero no se compil√≥ para Kmeros >31 (‚ÄºÔ∏èrevisar si es posible compilarlo para mayor n√∫mero de K-meros), ‚ùåClover se instal√≥ pero al parecer no funciona. ‚úÖIDBA funcion√≥ sin problema, aunque su desempe√±o no fue tan bueno. Revisar el ensamble con ‚ÄºÔ∏èGrasshoper, ‚ÄºÔ∏èSOAP de novo y ‚ÄºÔ∏èPlatanusB  en colab. 


## 7.2.1 SPAdes

La informaci√≥n de uso de [SPAdes](https://github.com/ablab/spades) se puede encontrar en el repositorio se√±alado. As√≠ mismo el art√≠culo que hace referencia a todos los pipelines que involucran el uso de SPAdes es el siguiente: [Prjibelski *et al.*, 2020](https://currentprotocols.onlinelibrary.wiley.com/doi/abs/10.1002/cpbi.102)
 
 De manera similar a cualquier otro ensamblador, el objetivo de SPAdes es construir secuencias continuas y precisas (Contigs y Scaffolds) a partir de secuencias cortas. 

SPAdes inicia el pipeline de ensamble construyendo una DBg a partir de las secuencias cortas. Despu√©s, la gr√°fica construida pasa por un procedimiento de simplificaci√≥n que involucra la eliminaci√≥n de filos (edges) err√≥neos. Tales filos son t√≠picamente ocasionados por errores o artefactos de secuenciaci√≥n. Una vez que la gr√°fica es simplificada, SPAdes mapea *short paired* y/o *long reads* de nuevo a la gr√°fica de ensamble usando esta informaci√≥n de alineamiento para realizar resoluci√≥n repetida o scaffolding usando el m√≥dulo exSPAder, el cual construir√° paths correctos y continuos para el genoma, siendo ensamblados en la gr√°fica de ensamble. 
 El protocolo b√°sico 1 es para aislamientos, mientras el protocolo 5 (que puede ser usado m√°s adelante) esta dedicado al descubrimiento de cluster de genes biosint√©ticos putativos. 
 
 En el caso de colab, primeramente se realiza la instalaci√≥n pertinente usando conda, tal como en la secci√≥n de [instalaci√≥n de paquetes](#1-instalacion-de-herramientas) en el apartado de [Instalacion de Megahit y SPAdes](#instalacion-de-megahit-y-spades).
 
 La l√≠nea de c√≥digo simplificada para hacer un primer ensamble en colab puede ser:
 
 ```python
 #prueba de instalaci√≥n
 ! spades.py --test
 
 #abre la ayuda para las flags
 ! spades.py -h
 
 #ensamble de aislamientos
 !spades.py -1 /content/drive/MyDrive/Analisis_Posdoc/PB016/Salidas_Pao/FastPR1_filtrada.fastq.gz -2 /content/drive/MyDrive/Analisis_Posdoc/PB016/Salidas_Pao/FastPR2_filtrada.fastq.gz -t 2 --isolate -o /content/drive/MyDrive/Analisis_Posdoc/PB016/Ensamble_Spades
 ```
 Se usa el flag -isolate para indicar que son aislamientos y que hay bastante profundidad de lo contrario el proceso se detiene (en el caso de la secuenciaci√≥n de lamgebio si se omite el proceso no se detiene).
 
SPAdes guarda puntos de control para reiniciar desde el √∫ltimo, por lo que para reiniciar se puede utilizar el siguiente bloque, no se debe olvidar colocar la carpeta de salida con la que ya se cuenta para que reanude tomando la informaci√≥n de esta:

```python
!spades.py --restart-from last -t 12 -o /content/drive/MyDrive/Analisis_Posdoc/PB16/Ensamble_SPAdes2 
```
En caso de que se requiera realizar el proceso en dos partes primero se puede usar el flag --only-error-correction, para que haga primero una correcci√≥n de errores, luego se puede usar el flag --only-assembler para que realice solo el ensamble y sea menos intensivo el uso de recursos. 

Existen otros flags que se pueden considerar.

## 7.2.2 Megahit
[Megahit](https://github.com/voutcn/megahit#basic-usage)
[Li *et al.*, 2015](https://pubmed.ncbi.nlm.nih.gov/25609793/)

Megahit utiliza gr√°ficos succintos DB (SdBG), los cuales son una representaci√≥n comprimida de los DBG. Una SdBG codifica una gr√°fica con m bordes o aristas (edges) un 0(m) bits y soporta 0(1) tiempos transversales de un v√≥rtice a sus vecinos. La implementaci√≥n de Megahit adiciona un vector-bit de una longitud m para marcar la validez de cada arista (as√≠ mismo se respalda la remoci√≥n de aristas eficientemente) y un v√©ctor auxiliar de 2kt bits (donde k es el tama√±o del k-mero y t es el n√∫mero de v√©rtices zero-indegree (o aislados)).

 > En teor√≠a de grafos, un v√©rtice o nodo es la unidad fundamental de la que est√°n formados los grafos. Un grafo no dirigido est√° formado por un conjunto de v√©rtices y un conjunto de aristas (pares no ordenados de v√©rtices), mientras que un grafo dirigido est√° compuesto por un conjunto de v√©rtices y un conjunto de arcos (pares ordenados de v√©rtices). En este contexto, los v√©rtices son tratados como objetos indivisibles y sin propiedades, aunque puedan tener una estructura adicional dependiendo de la aplicaci√≥n por la cual se usa el grafo; por ejemplo, una red sem√°ntica es un grafo en donde los v√©rtices representan conceptos o clases de objetos. Los dos v√©rtices que conforman una arista se llaman puntos finales ("endpoints", en ingl√©s), y esa arista se dice que es incidente a los v√©rtices. Un v√©rtice w es adyacente a otro v√©rtice v si el grafo contiene una arista (v,w) que los une. La vecindad de un v√©rtice v es un grafo inducido del grafo, formado por todos los v√©rtices adyacentes a v. 

A pesar de las ventajas que estas gr√°ficas representan, no es f√°cil su construcci√≥n por lo que megahit cuenta con un potente algoritmo paralelo para la construcci√≥n. Es decir puede explotar el paralelilsmo de las unidades GPU adaptando un algoritmo CX1 BWT

Antes de la construcci√≥n de la gr√°fica, todos los (k+1)-meros de las lecturas input son clasificadas y contadas y solo los (k+1)meros que se presentan al menos d (2 por defecto) veces se mantienen como kmeros s√≥lidos. Este m√©todo remueve muchas aristas espurias, pero puede ser riesgoso para el ensamble metagen√≥mico ya que especies de muy baja abundancia pueden haber sido secuenciadas a muy baja profundidad, por lo que tambi√©n se introduce una estrategia denominada mercy-kmer para recuperar dichos bordes (estos mercy-kmers se agregan a la gr√°fica para mejorar la contiguidad). 

Se implementa tambi√©n una estrategia de m√∫tiples tama√±os de kmeros, en el cual iterativamente se construyen m√∫ltiples SdBGs de un peque√±o a un mayor k. Mientras los kmeros peque√±os son favorables para filtrar bordes err√≥neos y rellenar gaps en regiones de baja cobertura, un mayor k es √∫til para resolver las repeticiones. En cada iteraci√≥n, se limpian los bordes potencialmente err√≥neos removiendo puntas, uniendo burbujas y removiendo bordes de baja cobertura local.

 En el caso de colab, primeramente se realiza la instalaci√≥n pertinente usando conda, tal como en la secci√≥n de [instalaci√≥n de paquetes](#1-instalacion-de-herramientas) en el apartado de [Instalacion de Megahit y SPAdes](#instalacion-de-megahit-y-spades).
 
 
 B√°sicamente, para el ensamble se usa el siguiente bloque de c√≥digo para lecturas *Paired end* 
 
 ```Phyton
#Probar lo siguiente para la direcci√≥n de las lecturas
#En shell es posible asignar as√≠
#!Lectura1=/content/drive/MyDrive/Analisis_Posdoc/PR69/salidas/HA1AB3SS04_S4_L1_R1_001.fastq.gz
#!Lectura2=/content/drive/MyDrive/Analisis_Posdoc/PR69/salidas/HA1AB3SS04_S4_L1_R2_001.fastq.gz

#!echo $Lectura1
#!echo $Lectura2

#en python podr√≠a ser 
lectura1 = path
lectura2 = path

print(lectura1)
print(lectura2)

# !megahit -1 pe_1.fq -2 pe_2.fq -o out #Revisar como se realizar√° el output en colab
!megahit -1 /content/drive/MyDrive/Analisis_Posdoc/PR69/salidas/HA1AB3SS04_S4_L1_R1_001.fastq.gz -2 /content/drive/MyDrive/Analisis_Posdoc/PR69/salidas/HA1AB3SS04_S4_L1_R2_001.fastq.gz -o /content/drive/MyDrive/Analisis_Posdoc/PR69/salidas/ensamble/HA1AB3SS041.megahit_asm

#en caso de que funcione la asignaci√≥n de variables 
!megahit -1 $Lectura1 -2 $Lectura2 -o ensamble_megahit_res

```
 
 Los contigs pueden encontrarse en el directorio de salida (para colab tenemos que establecerlo) como `final.contigs.fa`

```python
#Crear archivo zip desde carpeta de Colab
!zip -r /content/Ensamble_Megahit.zip /content/Ensamble_Megahit
```

para observar la gr√°fica de contigs

https://github.com/voutcn/megahit/wiki/An-example-of-real-assembly


```Phyton
!megahit_toolkit contig2fastg 99 k99.contigs.fa > k99.fastg
 ```
 
Algunos tips acerca del ensamble lo podemos encontrar [aqu√≠](https://github.com/voutcn/megahit/wiki/Assembly-Tips)

Para observar el principio del archivo de contigs y contar el n√∫mero aproximado podemos utilizar el siguiente bloque
```bash

%%bash
head contigs.fa
grep '>' ./directorio_salida/contigs.fa | wc -l 

```
 
## 7.2.3 ALGA 
.
Las herramientas de ensamble basadas en las gr√°ficas de De Brujin son las preferidas para lecturas cortas, debido a que despu√©s de la descomposici√≥n de las lecturas hay una p√©rdida de informaci√≥n. Pero tambi√©n al alto √≠ndice de error asociado con nuevas tecnolog√≠as que pobremente corresponde con las gr√°ficas de descomposici√≥n.
La superioridad de los ensambladores basados en dBG de acuerdo a tiempo y uso de memoria es bien conocido, pero otros ensambladores se desempe√±an mejor. Los algoritmos con la estrategia OLC dan contigs m√°s confiables pero con problemas significativos de memoria y tiempo. [ALGA](http://alga.put.poznan.pl/) ha mostrado desempe√±arse bien incluso en memoria y tiempo a pesar de ser de tipo OLC ([Swat *et al.*, 2021](https://academic.oup.com/bioinformatics/article/37/12/1644/6104855)).
 
 En el caso de este ensamblador, solo fue posible instalarlo y utilizarlo en WSL:
 
 ```bash 
 #para instalar
wget https://github.com/swacisko/ALGA/archive/refs/tags/1.0.3.tar.gz
tar zvxf 1.0.3.tar.gz

#nos movemos a la carpeta y compilamos
cd 1.0.3
mkdir build
cd build
cmake ..
make

## para ejecutar habr√≠a que usar ./ALGA

 ```
### 7.2.3.1 Correcci√≥n de errores con Musket
> Esta herramienta reci√©n se ha aplicado en WSL √∫nicamente.

Antes de ensamblar con ALGA se recomienda realizar correcci√≥n de errores con musket (se podr√≠a usar la correcci√≥n de SPAdes?)

Para utilizar [musket](https://musket.sourceforge.net/homepage.html) se recomienda primero su instalaci√≥n y compilaci√≥n (ya que esta basado en C) 

```bash
#Instalaci√≥n
wget https://sourceforge.net/projects/musket/files/musket-1.1.tar.gz/download
tar zvxf musket-1.1.tar.gz

cd musket-1.1

#compilaci√≥n
mkdir build
cd build
cmake ..
make

#para realizar la correcci√≥n de las lecturas
musket -omulti corrected -inorder /mnt/c/Users/adria/Downloads/PR69_Ensamble/FastPR1filtradodesduplicado.fastq /mnt/c/Users/adria/Downloads/PR69_Ensamble/FastPR2filtradodesduplicado.fastq 

```
### 7.2.3.2 Correcci√≥n de errores con SparkEC 
> A√∫n no se intenta su aplicaci√≥n

[SparkEC](https://bmcbioinformatics.biomedcentral.com/articles/10.1186/s12859-022-05013-1) es una herramienta que trabaja en paralelo, capaz de arreglar aquellos errores generados durante el proceso de secuenciaci√≥n. Su arquitectura es la misma utilizada por CloudEC (algoritmos MSA), sin embargo se eficient√≥ su desempe√±o, disminuyendo el tiempo de ejecuci√≥n y mejorando su utilidad evitando realizar ciertas tareas de forma manual.
Para su instalaci√≥n y aplicaci√≥n podemos dirigirnos al [repositorio SparkEC](https://github.com/UDC-GAC/SparkEC)

> Determinar si se puede aplicar el script en colab

Las lecturas ya corregidas se pueden ensamblar con ALGA:

 ```bash
 ./ALGA 
##Primero hay que descomprimir los archivos (ALGA no trabaja con .gz), la descompresi√≥n se puede hacer en colab
gzip -d /mnt/c/Users/adria/Downloads/PR69_Ensamble/FastPR1filtradodesduplicado.fastq.gz
gzip -d /mnt/c/Users/adria/Downloads/PR69_Ensamble/FastPR2filtradodesduplicado.fastq.gz

##luego ensamblamos

./ALGA --file1=/mnt/c/Users/adria/Downloads/PR69_Ensamble/FastPR1filtradodesduplicado.fastq --file2=/mnt/c/Users/adria/Downloads/PR69_Ensamble/FastPR2filtradodesduplicado.fastq --threads=14 --output=PR69algacontigs


##Agregar lo siguiente si se piensa que es de baja calidad (probablemente PR69 lo sea) y que 
##a√∫n tiene un alto n√∫mero de errores

--error-rate=0.02

 ```
 
 
 ## 7.2.4 Unicycler
 
‚ùóDado que Unicycler utiliza m√≥dulos de SPAdes, hasta este momento no se ha logrado (en colab o en WSL) que sea posible ensamblar, quiz√° se deba al path en el que se encuentra SPAdes üíî se continuar√° intentando. 

 ![image](https://user-images.githubusercontent.com/13104654/218617669-7611046b-f3e8-48d2-ad03-46a095c42621.png)
 [Unicycler](https://github.com/rrwick/Unicycler)
 
 
 ## 7.2.5 IDBA
Para instalar en colab:

```python
#Instalar IDBA (ensamblador)
!conda install -c bioconda IDBA -y 
``` 
Antes de comenzar a ensamblar hay que transformar las secuencias de formato fastq a formato fasta 

```python
#Convertir a formato fasta y filtrar N's
!fq2fa --merge --filter /content/drive/MyDrive/PR69/Salidas/MH005_filtered1.fastq /content/drive/MyDrive/PR69/Salidas/MH005_filtered2.fastq /content/drive/MyDrive/PR69/Salidas/MH005_filtered12.fa 

#Ensamble IDBA
!idba_ud -r /content/drive/MyDrive/PR69/Salidas/MH005_filtered12.fa -o /content/drive/MyDrive/PR69/Ensamble_IDBA/_idba_output --mink 51 --maxk 231 --step 10
```


 ## 7.2.6 Velvet
 
 Los primeros bloques permiten la instalaci√≥n de velvet pero no permiten una modificaci√≥n para k > 51
 
 ```python
 #Instalar Velvet (ensamblador)
!conda install -c bioconda Velvet -y 
 ```
 
 Para ensamblar en colab 
 ```python
 !velveth /content/drive/MyDrive/PR69/PR69_velvet 51 -fastq -separate -shortPaired /content/drive/MyDrive/PR69/Salidas/FastPR1filtradodesduplicado.fastq.gz /content/drive/MyDrive/PR69/Salidas/FastPR2filtradodesduplicado.fastq.gz
 ```
 
 ‚ùó‚ùó Buscar la manera de instalar y compilar modificando k 
 
 ## 7.2.7 PlatanusB
 https://github.com/rkajitani/Platanus_B
 
 ## 7.2.8 MeDuSa
 
 
 ## 7.2.9 Abyss
 
 [Repositorio Abyss](https://github.com/bcgsc/abyss#install-abyss-using-conda-recommended)
 
 ```bash
! conda install -c bioconda abyss
 
 # para ambientes dedicados 
! conda create -n abyss-env
! conda activate abyss-env
! conda install -c bioconda abyss
 
 export TMPDIR=/var/tmp
 
 abyss-pe k=25 name=test B=1G \
	in='test-data/reads1.fastq test-data/reads2.fastq'
  
  
  abyss-pe name=ecoli k=96 B=2G in='reads1.fa reads2.fa'


abyss-pe k=96 B=2G name=ecoli lib='pea peb' mp='mpc mpd' \
	pea='pea_1.fa pea_2.fa' peb='peb_1.fa peb_2.fa' \
	mpc='mpc_1.fa mpc_2.fa' mpd='mpd_1.fa mpd_2.fa'

 ```
   Long-distance mate-pair libraries may be used to scaffold an assembly. Specify the names of the mate-pair libraries using the parameter mp. The scaffolds will be stored in the file ${name}-scaffolds.fa. Here's an example of assembling a data set with two paired-end libraries and two mate-pair libraries. Note that the names of the libraries (pea, peb, mpa, mpb) are arbitrary. 
   
 ## 7.2.10 GrassHopper
 
 [Grasshopper](https://sourceforge.net/projects/grasshopper-assembler/)
 [Swiercz *et al.*, 2018](https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0202355)
 
 # 7.3 Calidad de Ensamble

**El problema de la validaci√≥n del genoma**

Aunque se sabe que es muy posible que haya errores en un ensamble de genoma, existe una falta de programas que detecten autom√°ticamente dichos errores o que asignen un *score* de confianza en diferentes regiones del mismo. 

Este problema es particularmente importante ya que la aplicaci√≥n de las tecnolog√≠as de secuencia paralelas, casi siempre generan lecturas cortas y as√≠, incrementan el riesgo de un ensamblado incorrecto (*misassembly*).

Un gran n√∫mero de estos errores son causados por repeticiones. Los ensambladores pueden ser confundidos por pseudo-sobrelapes entre estas lecturas repetidas (de diferentes copias de repeticiones casi id√©nticas) y colocarlas juntas. T√≠picamente pueden inducirse dos tipos de *misassemblies*: el colapso de repeticiones y el rearreglo a gran escala.

La identificaci√≥n y separaci√≥n de estas repeticiones colapsadas ha sido estudiada como un problema computacionalmente independiente **el problema de separaci√≥n de repeticiones** y se han propuesto diversas estrategias combinatoriales y probabil√≠sticas para resolverlo, adem√°s el uso de lecturas pareadas puede mejorar esta separaci√≥n.

El rompecabezas del ensamble casi siempre contiene muchas piezas que son similares en color y forma (repeticiones) y sin tener una idea de como se ver√° al final. Los errores de ensamble pueden pensarse como piezas que son forzadas a unirse pero que no encajan al final. Se puede definir el termino encajar en el sentido categ√≥rico y probabil√≠stico, as√≠, en el caso categ√≥rico, los errores de ensamble pueden ser identificados por secuencias que no pueden ser colocadas en el genoma, estas representan secuencias singletones, pares cuya colocaci√≥n es inconsistente con la librer√≠a o sobrelapes cuya composici√≥n difiere m√°s de lo que puede ser explicado por errores de secuenciaci√≥n. En el sentido probabil√≠stico, los errores de ensamble corresponden a regiones del genoma donde el tejido del shotgun es inconsistente con el proceso aleatorio usado para generar  ese secuenciado. Por ejemplo las secciones de un ensamble donde las lecturas se ‚Äúamontonan‚Äù m√°s de lo esperado puede indicar el colapso (coensamblaje) de m√∫ltiples copias de una repetici√≥n gen√≥mica. Este ajuste probabil√≠stico conduce a un elegante formulaci√≥n del ensamble del genoma como la tarea de identificar un mosaico de lecturas que mejor coincidan con las propiedades del proceso aleatorio utilizado para generar los datos.

El aseguramiento de la calidad a√∫n es m√°s dif√≠cil debido al avance continuo de las herramientas de secuenciaci√≥n, as√≠ que cada vez que los datos cambian, representa un nuevo problema para la programaci√≥n

En ausencia de un genoma de referencia de alta calidad, los nuevos ensambles casi siempre se evaluan dependiendo del n√∫mero de scaffolds y contigs requeridos para representar el genoma, la proporci√≥n de lecturas que puede ser ensamblada, la longitud absoluta de los contigs y scaffolds y la longitud de los contigs y scaffolds relativos al tama√±o del genoma.

## Criterios 3C
### Contiguidad
La m√©trica m√°s com√∫nmente utilizada es el üí° **N50**, si todos los contigs en un ensamble se ordenan por longitud, esta m√©trica es la longitud del contig  m√°s peque√±o al 50% de las bases ensambladas.

![Screenshot 2023-08-07 224846](https://github.com/JannaColt/MineriaGen/assets/13104654/045c49a6-cb37-4aef-9f29-ba80db438387)

üî¥‚ö†Ô∏è S√≥lo indica continuidad de bases.

üî¥‚ö†Ô∏è F√°cil de manipular, no es una medida de precisi√≥n del ensamble, debe usarse con precauci√≥n.

üî¥‚ö†Ô∏è No es significativa para diferentes tama√±os de ensamble (no comparable entre especies incluso en el mismo genoma)

üî¥‚ö†Ô∏è Sesgado si se excluyen secuencias cortas (lo que casi siempre ocurre)


Herramientas bien establecidas pueden producir ensambles con ‚¨ÜÔ∏èN50, sin embargo, esto puede alcanzarse removiendo k-meros repetidos de bajo coverage (Sacrificando complejidad por contiguidad). Uno puede extender el N50 pero puede carecer de genes conservados.

**NG50** por otro lado, de forma similar a N50 corresponde al contig m√°s peque√±o pero al 50% del tama√±o del genoma conocido o estimado, por lo que permite comparaciones significativas entre diferentes ensambles. Se pueden filtrar contigs peque√±os sin afectar el valor.

Por su parte **L50** corresponde al conteo de contigs en el 50% del ensamble
 Si graficamos la curva Nx, nos dar√≠a una mejor visualizaci√≥n ded la continuidad.

### **C**ompletness
- Revisando el tama√±o del ensamble
- Nucle√≥tidos conocidos vs desconocidos (esperamos un ensamble sin N's)
- Genes "n√∫cleo" Aseguramiento cuantitativo del genoma ensamblado basado en expectativas evolutivas informadas de  contenidos de genes casi universaldes de ort√≥logos de copia √∫nica. 
- Contenido de k-meros ensamblados
- Mapeo de lecturas y *coverage* de ensamblado

(podemos usar BUSCO y  Merqury: reference-free quality, completeness, and phasing assessment for genome assemblies)

### Correctness (Fidelidad)
Errores que se presentan en el ensamble, proporci√≥n del ensamble que est√° libre de errores como:

‚ú¥Ô∏è Indels / SNPs

‚ú¥Ô∏è Mis-joins

‚ú¥Ô∏è Compresiones repetidas

‚ú¥Ô∏è Duplicados innecesarios

‚ú¥Ô∏è Rearreglos.

Pero se realiza contra una referencia, y a veces no contamos con alguna adecuada. Para ello podemos usar dotplots : 
MUMmer dotplot
Chromeister

Algunos pueden utilizar las siguientes estrategias para validar:
‚ñ∂Ô∏è BUSCO/CEGMA para la b√∫squeda de los genes n√∫cleo

‚ñ∂Ô∏è Mapear lecturas RNASeq y unigenes derivados del ensamble de trasncriptoma

‚ñ∂Ô∏è Mapear prote√≠nas de especies cercanamente relacionadas    

‚ñ∂Ô∏è Mapear lecturas constituyentes que fueron usadas para formar el ensamble y revisar su profundidad y rastreabilidad

‚ñ∂Ô∏è Distribuci√≥n de NGx (10, 50, 70, 90, etc)

‚ñ∂Ô∏è Distribuci√≥n de longitud de contigs

‚ñ∂Ô∏è Revisar la presencia de contigs duplicados y otros contaminantes (la forma m√°s f√°cil es subir el genoma a NCBI)

‚ñ∂Ô∏è Bases constituyentes del ensamble

[Art√≠culo: *De Novo* Genome assembly: what every biologist should know](http://genetica.uab.cat/makingsensegenomicsdata/MakingSenseGenomicData_Reading.pdf)
 https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2397507/pdf/gb-2008-9-3-r55.pdf

 ## 7.3.1 QUAST

```python
#Instalaci√≥n de Quast
!conda install -c bioconda quast -y

#  Correr Quast para ensambles 
!quast.py -o /content/drive/MyDrive/PR69/Estadistica_Ensamble /content/drive/MyDrive/PR69/Ensamble_SPAdes/contigs.fasta /content/drive/MyDrive/PR69/Ensamble_Megahit4/final.contigs.fa ... <dir de cada ensamble>
```

 ## 7.3.2 CheckM
 

## 7.3.3 BUSCO 

 [BUSCO](https://busco.ezlab.org/) es una herramienta con base en expectativas evolutivamente-informadas del contenido de genes ort√≥logos single-copy casi-universales, la m√©trica BUSCO es complementaria a las m√©tricas t√©cnicas como N50.

Primero instalamos con conda tratando de que se instalen todos los programas por defecto que se necesiten (esta instalaci√≥n no permite que se use busco).
```bash
! conda install -c conda-forge -c bioconda -c defaults busco -y -vv
```

Despu√©s hay que hacer lo siguiente.

```bash
%cd /content/drive/MyDrive/Busco 
! ls
```


 ```bash
! git clone https://gitlab.com/ezlab/busco.git
 ```

 ```bash
%cd /content/drive/MyDrive/PR69/Busco/busco
! python3 setup.py install
 ```

 ```bash
pd.__version__ 
 ```

 ```bash
sys.path.append('/content/drive/MyDrive/PR69/Busco')
 ```

 ```bash
%cd /content/drive/MyDrive/PR69/Busco/busco/src
! busco -i /content/drive/MyDrive/PR69/Ensamble_SPAdes_MH005_fd/contigs.fasta -l bacteria_odb10 -o anotbusco_PR69 -m genome
 ```

> [!IMPORTANT]
> Una vez que se ha clonado BUSCO en el drive, y que no se ha borrado, podemos omitir el paso de clonado de repositorio y √∫nicamente hacer los pasos de antes y despu√©s.

> [!TIP]
> :bulb: üîÜ Para los que est√°n usando la cuenta del laboratorio, el repositorio clonado de BUSCO se encuentra en el path: `/content/drive/MyDrive/Cynthia/CH619_CC/Anotacion/BUSCO`

Si resulta complicado, se puede realizar la anotaci√≥n usando el web service de [Galaxy](https://usegalaxy.org/)

### 7.3.3.1 Configuracion en Galaxy

1. En el panel de herramientas (primer panel a la izquierda), buscar **Genomic Analysis** -> **Annotation** -> **BUSCO**
2. En el panel central de par√°metros de herramientas subir el ensamble del genoma (este son los contig en formato FASTA) 
   
![image](https://github.com/JannaColt/MineriaGen/assets/13104654/93d5fe5e-96d9-4a49-9000-297c3a0aa1af)

luego en los par√°metros seleccionar 

:red_circle: en **Lineage data source** -> **Download lineage data** 

:red_circle: en **Mode** -> **Genome assembly (DNA)**

:red_circle: en **Generate miniprot output** se puede dejar a su elecci√≥n, si se activa tendremos un output tabular con los genes anotados y su posici√≥n (gff)

:red_circle: en  **Use Augustus instead of Metaeuk** seleccionamos Augustus, ya que metaeuk es exclusivo de eucariotas.

:red_circle: en **Auto-detect or select lineage?** seleccionamos auto-detect, si sabemos el linaje podemos colocarlo aqu√≠

:red_circle: en **auto-lineage group'*'** podemos dejar el auto lineage de todos los grupos taxon√≥micos o el prokaryotes 

:red_circle: en **Which outputs should be generated** hay que se√±alar todos los output que queremos, los cuales pueden ser: un output con el resumen, una listado con los genes perdidos, un gff con datos de la anotaci√≥n y una imagen con el resumen de genes n√∫cleo.

3. Una vez establecidos todos los par√°metros, podemos correr la herramienta. Si existe alg√∫n fallo en algunos de los archivos de salida podemos editar los atributos y presionar auto-detect para que se haga la correcci√≥n autom√°tica.   

*¬øQu√© me dice el an√°lisis de busco?*

El programa nos proporciona un aseguramiento de la completitud en t√©rminos de contenido degenes esperados de un ensamble o conjunto de genes anotados. Los resultaados son simplificados en categor√≠as de completos y de copia √∫nica, completos y duplicados, fragmentados o Busco's perdidos (genes marcadores). 

Los resultados de Busco hacen sentido en el contexto de la biolog√≠a del organismo. Entendiento que los genes duplicados o perdidos pueden ser de origen t√©cnico o biol√≥gico. Por ello un alto nivel de duplicaci√≥n puede ser explicado por un evento de duplicaci√≥n reciente (biol√≥gicamente hablando) o un ensamble quim√©rico de haplotipos (t√©cnico).

:high_brightness: **Completos** 
Si encontramos genes completos, ya sea de copia √∫nica o duplicados, los BUSCO han coincidido con Score suficiente, dentro del rango de Scores esperados y en longitud en cuanto a los alineamientos del perfil BUSCO. Si un ort√≥logo no est√° presente en el input, o est√° parcialmente presente (altamente fragmentado), y un hom√≥logo de alta identidad est√° presente en longitud completa, es posible que este hom√≥logo haya sido confundido y err√≥neamente identificado como el BUSCO completo. Los l√≠mites del score est√°n optimizados para minimizar esta posibilidad pero a√∫n puede ocurrir.

:low_brightness: **Fragmentados**

Si encontramos genes fragmentados, las coincidencias BUSCO han *Scoreado* dentro del rango de scores pero no dentro del rango de longitud de alineamientos. En ensambles de genomas esto podr√≠a indicar que o el hen est√° parcialmente presente o que el paso de la b√∫squeda de la secuencia y predicci√≥n de genes ha fallado en producir un modelo de longitud completa del gen incluso pensando que el gen est√° completamente presente en el ensamble. Algunos otros pueden a√∫n estar completos pero pueden ser muy divergentes o tener estructuras gen√©ticas complejas, haci√©ndolos muy dif√≠cil de localizar y predecir al 100%. 

‚ÅâÔ∏è :full_moon: **Missing *(perdidos)***

Esto significa que, o no hubo coincidencias significativas, o que las coincidencias BUSCO se presentaron con un score por debajo del rango de scores para el perfil BUSCO. Esto puede indicar que los ort√≥logos est√°n perdidos, o que el paso de b√∫squeda de secuencia fall√≥ al identificar cualquier coincidencia significativa, o que el paso de predicci√≥n de genes fall√≥ al producir incluso un modelo gen√©tico parcial que podr√≠a haber sido reconocido como una coincidencia BUSCO fragmentada. Algunos missing BUSCOs de los aseguramientos de ensambles de genoma podr√≠an as√≠ estar parcialmente presentes, e incluso posible pero dif√≠cilmente completos, solo que son demasiado divergentes o tienen muy complejas estructuras gen√©ticas haciendo dif√≠cil su localizaci√≥n o predicci√≥n correcta o incluso parcial.


### 7.3.3.2 Archivos de salida en Colaboratory

Actualmente el perfil de BUSCOs de colaboratory est√° limitado, probablemente debido a que se tiene que definir el path hacia Augustus. Ver Nota

![image](https://github.com/JannaColt/MineriaGen/assets/13104654/45d1a0b8-85ed-4757-a95b-e79b8e0ca0ac)


![image](https://github.com/JannaColt/MineriaGen/assets/13104654/c4a0cbc3-5bfe-4cde-9566-97cc85a70832)


### 7.3.3.3 Archivos de salida en Galaxy

Los archivos de salida de Galaxy depender√°n de lo que se haya marcado en el √∫ltimo punto del paso 2 en [Configuraci√≥n en Galaxy](#7331-configuracion-en-galaxy)
 
![image](https://github.com/JannaColt/MineriaGen/assets/13104654/c9b4ee98-c07f-4676-85dc-a15307ffe3a7)


> [!NOTE]
> para definir un path ya que colab no acepta export
en entorno normal de linux-unix
```bash
export PATH="/path/to/AUGUSTUS/augustus-3.2.3/bin:$PATH"
export PATH="/path/to/AUGUSTUS/augustus-3.2.3/scripts:$PATH"
export AUGUSTUS_CONFIG_PATH="/path/to/AUGUSTUS/augustus-3.2.3/config/"
```
Probablemente esto sirva:
```bash
! echo $PYTHONPATH
%env PYTHONPATH="$/env/python:/content/gdrive/My Drive/Colab Notebooks/path/to/AUGUSTUS/augustus-3.2.3/config/src"
! echo $PYTHONPATH
```

 
 # 7.4 Metaensamblado
 
 
 ## 7.4.1 MAC
 
 ### MAC
Modelo algebraico de adyacencia y clasificaci√≥n.
Considerar que hay que tener instalado [Mummer](https://colab.research.google.com/drive/1qHrgEQ-rsSG5IxC_BcHzs15cRPEcwwXs#scrollTo=OdaJWjlE0rho&line=2&uniqifier=1) antes de iniciar 

```python
%cd /content/drive/MyDrive/Analisis_Posdoc/PR69/Ensamble/Metaensamble/MACassembler
! ls
## Clonamos el repositorio con c√≥digo, en la carpeta MACassembler creada

! git clone https://github.com/bioinfomaticsCSU/MAC.git
# Hay que vincular el directorio donde clonamos el repositorio 
sys.path.append('/content/drive/MyDrive/Analisis_Posdoc/PR69/Ensamble/Metaensamble/MACassembler/MAC')

%cd /content/drive/MyDrive/Analisis_Posdoc/PR69/Ensamble/Metaensamble/MACassembler/MAC
!ls

##compilamos usando g++
%%bash
#g++ MAC2.0.cpp -o MAC2

#o si no funciona 
g++ -std=c++11 -std=gnu++11 MAC2.0.cpp -o MAC2.0

#Para correr el programa: 
#Primero hay que posicionarnos en el directorio que contiene las carpetas input, output y temp (ya que no admite path), (tomar en cuenta que la carpeta
#input debe contener los contigs de referencia y query)
#%cd /content/drive/MyDrive/Analisis_Posdoc/PR69/Ensamble/Metaensamble/MACassembler/MAC
%%bash
./MAC2.0 Mh4finalcontigs.fa SPdcontigs.fa
```
 
 ## 7.4.2 Metassembler
 
 #Primero instalamos las herramientas que usar√° metassembler (omitir si ya est√°n instaladas)
! conda install -c bioconda mummer -y
! conda install -c bioconda bowtie2 -y
! conda install -c bioconda samtools -y

 # 7.5 Mejoramiento del Ensamble
 
 ## 7.5.1 SASpector

[Art√≠culo](https://academic.oup.com/bioinformatics/article/38/10/2920/6564223)

[Repositorio](https://github.com/LoGT-KULeuven/SASpector)
 
 ## 7.5.1 Gap predict

 [Art√≠culo](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8772386/pdf/nihms-1763064.pdf)

 [Repositorio](https://github.com/bcgsc/GapPredict)
 
 ## 7.5.2 Gap closer

 [Art√≠culo](https://academic.oup.com/gigascience/article/9/9/giaa094/5902284?login=false)

 [Repositorio](https://github.com/BGI-Qingdao/TGS-GapCloser)
 
 ## 7.5.3 Improvement assembly
 
[Art√≠culo](https://www.microbiologyresearch.org/content/journal/mgen/10.1099/mgen.0.000083)

[Repositorio](https://github.com/sanger-pathogens/assembly_improvement)
 
 ## 7.5.4 Figbird
 
 [Art√≠culo](https://academic.oup.com/bioinformatics/article/38/15/3717/6613135?login=false)

 [Repositorio](https://github.com/SumitTarafder/Figbird)

## 7.5.5 GFinisher

[art√≠culo](https://www.nature.com/articles/srep34963)

[repositorio](https://gfinisher.sourceforge.net/)

## 7.5.6 Filtrado de contigs
### 7.5.6.1 con BBMAP

```bash
#Instalamos BBMAP
! conda install -c bioconda bbmap -y
```

```Bash
# arriba 1000pb
#Para eliminar los contigs que son menor de 1000pb se puede usar bbmap (hay que instalar bbmap primero)
#en in se coloca la direcci√≥n del ensamble que vas a filtrar, en out le vas a poner la direcci√≥n donde quieres que quede y el nombre de salida
#en min length es la longitud de los que quedan filtrados

! reformat.sh in=/content/drive/MyDrive/Cynthia/CH230_CC/Ensamble_MEGAHIT/CH230_Ensamble_MEGAHIT_final.contigs.fa out=/content/drive/MyDrive/Cynthia/CH230_CC/Limpieza_Ensambles/Limpieza_Ensamble_MEGAHIT/CH230_filteredMAC1000.fasta minlength=1000

```

### 7.5.6.2 con KBase

[Art√≠culo](https://www.nature.com/articles/nbt.4163)

[repositorio app](https://github.com/kbaseapps/kb_assembly_compare/tree/87b74d0dba8cb19dcbd3e7782d4f73a5eaeff76b/ui/narrative/methods/run_filter_contigs_by_length)

Se necesita registrarse a Kbase
[App](https://kbase.us/applist/apps/kb_assembly_compare/run_filter_contigs_by_length/release)
 
 # 8. ANOTACION
 
 ## 8.1 Prokka
 
 ### 8.1.1 Archivos de salida
 Una vez que Prokka ha terminado de anotar, se examina cada uno de los archivos, lo cual se puede realizar utilizando comandos b√°sicos de python.
 Los archivos de salida son los siguientes:
 
 Los archivos GFF y GBK contienen toda la informaci√≥n acerca de las caracter√≠sticas anotadas (en diferentes formatos).
 El .txt contiene un resumen del n√∫mero de caracter√≠sticas anotadas
 El archivo .faa contiene las secuencias de prote√≠nas de los genes anotados
 El archivo .ffn contiene las secuencias nucleot√≠dicas de los genes anotados

 ```bash
#Instalaci√≥n de Prokka
# !conda install -c bioconda prokka -y 
!conda install -c conda-forge -c bioconda -c defaults prokka
```
 ```bash
!prokka
```

 ```bash
#Correr Prokka 
!prokka --prefix PR69Prokka --locustag PR69 /content/drive/MyDrive/PR69/Ensamble_SPAdes_MH005_fd/contigs.fasta
``` 
 ```bash
#Crear archivo zip desde carpeta de Colab
!zip -r /content/Prokka_PR69_mejor.zip /content/PR69Prokka
```

### 8.1.2 Visualizaci√≥n de caracter√≠sticas anotadas usando JBrowse

Una forma de visualizar el draft del genoma es utilizando herramientas como *JBrowse genome viewer*

First, we have to make a JBrowse file. Then, we can view it within Galaxy.

## 8.2 PRODIGAL 

## 8.3 PATRIC

La anotaci√≥n la podemos realizar en PATRIC o BV-BRC
[Web service](https://www.bv-brc.org/)

## 8.4 RAST

# 9. ANOTACI√ìN FUNCIONAL

## 9.1 EggNOG Mapper

[Art√≠culo](https://academic.oup.com/mbe/article/38/12/5825/6379734)

[Servicio online](http://eggnog-mapper.embl.de/)

[Repositorio](https://github.com/eggnogdb/eggnog-mapper)

![Diagrama de *workflow*](https://github.com/JannaColt/MineriaGen/assets/13104654/96932911-9cbe-4a7d-9480-286736727738)

## 9.2 FunMappOne 

[Art√≠culo](https://bmcbioinformatics.biomedcentral.com/articles/10.1186/s12859-019-2639-2)

[Repositorio](https://github.com/Greco-Lab/FunMappOne)

## 9.3 MicrobeAnnotator

[Art√≠culo](https://bmcbioinformatics.biomedcentral.com/articles/10.1186/s12859-020-03940-5)

[Repositorio](https://github.com/cruizperez/MicrobeAnnotator)

## 9.4 Deep Learning
[Art√≠culo](https://pubmed.ncbi.nlm.nih.gov/37010293/)

![DL-functionalAnnotation](https://github.com/JannaColt/MineriaGen/assets/13104654/6535dca0-8913-44b1-9b54-dbb45b829bc6)

## 9.5 MicroScope

[Art√≠culo](https://academic.oup.com/nar/article/48/D1/D579/5606622)
[Plataforma](https://mage.genoscope.cns.fr/microscope/home/index.php)

# 10. MINER√çA GEN√ìMICA

## 10.1 AntiSMASH

[Art√≠culo](https://academic.oup.com/nar/article/51/W1/W46/7151336)

[Web Tool](https://antismash.secondarymetabolites.org/#!/start)

## 10.2 DeepBGC

[Art√≠culo](https://academic.oup.com/nar/article/47/18/e110/5545735)

[Repositorio](https://github.com/Merck/deepbgc)

## 10.3 e-DeepBGC

[Art√≠culo](https://www.sciencedirect.com/science/article/abs/pii/S0022283622001772)

 [repositorio]()

## 10.4 DecRippter

[Art√≠culo](https://journals.plos.org/plosbiology/article?id=10.1371/journal.pbio.3001026)

[Repositorio](https://github.com/Alexamk/decRiPPter)

## 10.5 DeepRipp

[Art√≠culo](https://www.pnas.org/doi/10.1073/pnas.1901493116)

[Repositorio](https://github.com/magarveylab/NLPPrecursor)

## 10.6 GECCO

[Art√≠culo](https://www.biorxiv.org/content/10.1101/2021.05.03.442509v1)

[Repositorio](https://github.com/zellerlab/GECCO)

## 10.7 BigCARP

[Art√≠culo](https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1011162)

[Repositorio](https://github.com/microsoft/bigcarp)

## 10.8 Otros

[Deep Self supervised Learning](https://www.biorxiv.org/content/10.1101/2022.07.22.500861v1.full.pdf)


# 11. AN√ÅLISIS FILOGEN√âTICOS

## 11.1 PhaME 

[Art√≠culo](https://www.nature.com/articles/s41598-020-58356-1)

[Repositorio](https://github.com/LANL-Bioinformatics/PhaME)

[m√°s...](https://phame.readthedocs.io/en/latest/)

 pipeline: TORMES https://github.com/nmquijada/tormes








Ejemplo Metodolog√≠a art√≠culo
Experimental Design, Materials and Methods
PR69 was previously isolated and identified with 16SrRNA with GenBank accessionnumber #####. Genomic DNA was extracted using the GenomicDNA Purification Kit(NewEnglandBiolabs.). IlluminaHiSeq4000 paired-end (2√ó151bp) sequencing of PR69 was performed by Langebio (CINVESTAV). The library was processed using the XXXX Library Preparation Kit (96samples (Illumina,Inc.,SanDiego,CA,USA). Total sequencing reads 5,130,218 of 4,447,316 were mapped. Aftermapping, Sambamba[10] and SAMTools[11] were respectively used to remove duplicated reads and identify variants. 
Thereadswereassembledinto58contigs,aGCcontentof41.60%usingGCEAssembler(version1.2;https://cge.cbs.dtu.dk/services/Assembler/)[12].TheassembleddatawasannotatedusingRASTrapidannotationusingsubsystemtechnologyversion2.0[6‚Äì8].BacterialsecondarymetabolitebiosynthesisgeneclusterswereidentifiedandannotatedbyantiSMASHversion5.0and6.0usingassembledfastafileoutput[







